{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "870b139a-fb2a-44a4-939b-1dc6965060e9",
   "metadata": {},
   "source": [
    "# Introduction to NLP Fundamentals in TensorFlow\n",
    "\n",
    "NLP has the goal of deriving information out of natural language (could be sequences text of speech).\n",
    "\n",
    "Another common term for NLP problems is sequence to sequence problems (seq2seq)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "597a352f-dbaf-467b-aefb-2895e6b9778a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 4060 Ti (UUID: GPU-57ca58af-5e07-5a3f-6ef1-efc944d47947)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b20e9cd-8a5d-4dd3-94eb-c79d92b42924",
   "metadata": {},
   "source": [
    "## Get helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bae80ce-e3da-4168-9445-d583a4b065a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-03 13:54:16--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/refs/heads/main/extras/helper_functions.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10246 (10K) [text/plain]\n",
      "Saving to: ‘helper_functions.py’\n",
      "\n",
      "helper_functions.py 100%[===================>]  10.01K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-11-03 13:54:17 (60.7 MB/s) - ‘helper_functions.py’ saved [10246/10246]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm -rf helper_functions.py\n",
    "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/refs/heads/main/extras/helper_functions.py\n",
    "from helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, compare_historys, walk_through_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825c6411-8596-4bc4-bdd3-c096c2ca8d97",
   "metadata": {},
   "source": [
    "## Get a text dataset\n",
    "\n",
    "The dataset we're going to be using is Kaggle's introduction to NLP dataset (text samples of Tweets labelled as disaster or not disaster).\n",
    "\n",
    "Og dataset: https://www.kaggle.com/code/philculliton/nlp-getting-started-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be36c8ff-9e50-4214-9b59-d79770b2b8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-03 13:54:21--  https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 216.58.209.187, 216.58.211.251, 216.58.210.155, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|216.58.209.187|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 607343 (593K) [application/zip]\n",
      "Saving to: ‘nlp_getting_started.zip’\n",
      "\n",
      "nlp_getting_started 100%[===================>] 593.11K  --.-KB/s    in 0.08s   \n",
      "\n",
      "2024-11-03 13:54:21 (7.22 MB/s) - ‘nlp_getting_started.zip’ saved [607343/607343]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm -rf sample_submission.csv\n",
    "!rm -rf test.csv\n",
    "!rm -rf train.csv\n",
    "\n",
    "!wget https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\n",
    "\n",
    "# Unzip data\n",
    "unzip_data(\"nlp_getting_started.zip\")\n",
    "!rm -rf nlp_getting_started.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9052cb69-cbef-4eee-8d42-7013d018208d",
   "metadata": {},
   "source": [
    "## Visualizing a text dataset\n",
    "\n",
    "To visualize our test samples, we first have to read them in, one way to do so would be to use Python: https://readlpython.com/read-write-files-python\n",
    "\n",
    "Or with pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e86e4334-2bf9-43d5-8dd8-09b343919a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f654321-bade-4246-9acc-745774eaf9ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "348b44d7-8c32-4934-bed4-501d6d354d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>3796</td>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So you have a new weapon that can cause un-ima...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>3185</td>\n",
       "      <td>deluge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>7769</td>\n",
       "      <td>police</td>\n",
       "      <td>UK</td>\n",
       "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>191</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aftershock back to school kick off was great. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>9810</td>\n",
       "      <td>trauma</td>\n",
       "      <td>Montgomery County, MD</td>\n",
       "      <td>in response to trauma Children of Addicts deve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      keyword               location  \\\n",
       "2644  3796  destruction                    NaN   \n",
       "2227  3185       deluge                    NaN   \n",
       "5448  7769       police                     UK   \n",
       "132    191   aftershock                    NaN   \n",
       "6845  9810       trauma  Montgomery County, MD   \n",
       "\n",
       "                                                   text  target  \n",
       "2644  So you have a new weapon that can cause un-ima...       1  \n",
       "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
       "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
       "132   Aftershock back to school kick off was great. ...       0  \n",
       "6845  in response to trauma Children of Addicts deve...       0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle training dataframe\n",
    "train_df_shuffled = train_df.sample(frac=1, random_state=42)\n",
    "train_df_shuffled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa84bdcc-263a-49da-ad41-3b05ea782b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What does the test dataframe look like\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18fe18fc-9650-4d25-be31-07565158a43b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    4342\n",
       "1    3271\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cc80a9b-b656-4f9a-ae38-603198de78b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 3263)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How manny total samples?\n",
    "len(train_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "654b1ffd-0583-4d51-ba77-e225118a406a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 1 (real disaster) \n",
      "Text: Oil and Gas Exploration http://t.co/PckF0nl2yN\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not real disaster) \n",
      "Text: Riot Kit Bah - part of the new concept Gear coming for Autumn/Winter\n",
      "#menswear #fashion #urbanfashionÛ_ https://t.co/cCwzDTFbUS\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not real disaster) \n",
      "Text: @MelRises @gayler1969 @wwwbigbaldhead @jessienojoke @melissaross9847 if my Monty Python is up to date as bloody far as he wants to go.\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not real disaster) \n",
      "Text: @_STiiiLO I still got video of u demolished\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster) \n",
      "Text: Japan Aogashima Volcano. By Unknown - Check It Out! http://t.co/OegFQBIqIq\n",
      " #Aogashima #Japan #photography #Volcano\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's visualize some random training examples\n",
    "import random\n",
    "random_index = random.randint(0, len(train_df)-5)\n",
    "for row in train_df_shuffled[[\"text\", \"target\"]][random_index:random_index+5].itertuples():\n",
    "    _, text, target = row\n",
    "    print(f\"Target: {target}\", \"(real disaster)\" if target > 0 else \"(not real disaster)\", f\"\\nText: {text}\\n\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e760a2-eadc-4468-8775-9acfede3747c",
   "metadata": {},
   "source": [
    "## Split data into training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71c3ae18-5b5a-41af-9a30-07b14c785d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "    train_df_shuffled[\"text\"].to_numpy(),\n",
    "    train_df_shuffled[\"target\"].to_numpy(),\n",
    "    test_size=0.1, # use 10% of data for validation\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24c3ab01-e574-4b81-82a1-606f4c99bf81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 6851, 762, 762)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the lengths\n",
    "len(train_data), len(train_labels), len(val_data), len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34686dd5-b34c-4bde-abf1-45dc11f5672f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['@mogacola @zamtriossu i screamed after hitting tweet', 0],\n",
       "       ['Imagine getting flattened by Kurt Zouma', 0],\n",
       "       ['@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
       "        1],\n",
       "       [\"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
       "        0],\n",
       "       ['Somehow find you and I collide http://t.co/Ee8RpOahPk', 0],\n",
       "       ['@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n",
       "        1],\n",
       "       ['destroy the free fandom honestly', 1],\n",
       "       ['Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n",
       "        0],\n",
       "       ['@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
       "        1],\n",
       "       ['Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt',\n",
       "        1]], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the first 10 samples\n",
    "import numpy as np\n",
    "np.transpose((train_data[:10], train_labels[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54138261-8b1b-42aa-bdfa-f3ed55472b77",
   "metadata": {},
   "source": [
    "## Converting text into number\n",
    "\n",
    "When dealing with a text problem, one of the first things you'll have to do before you can build a model is to convert your text to numbers.\n",
    "\n",
    "There are a few ways to do this, namely:\n",
    "* Tokenization -  direct mapping of token( a token could be a word or a character) to number\n",
    "* Embedding - create matrix of feature vector for each token (the size of the feature vector can be defined and this embedding can be learned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11cc820-763f-4372-b656-8a60d460a012",
   "metadata": {},
   "source": [
    "### Text vectorization (tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b58b0ad9-70bb-4076-a751-59d8cb1e086d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33d0b8fa-fbf8-4dd9-ba62-a3bec8b3deca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the default TextVectorization parameters\n",
    "text_vectorizer = TextVectorization(\n",
    "    max_tokens=None,\n",
    "    standardize='lower_and_strip_punctuation',\n",
    "    split='whitespace',\n",
    "    ngrams=None,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=None,\n",
    "    pad_to_max_tokens=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7730344-88c0-4779-b1b9-e5f9ec77f5ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the average number of tokens (words) in the training tweets\n",
    "round(sum([len(i.split()) for i in train_data])/len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b2c28e5-444f-49f5-b473-5a1972e681bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup text vectorization variables\n",
    "max_vocab_length = 10000\n",
    "max_length = 15\n",
    "\n",
    "text_vectorizer = TextVectorization(\n",
    "    max_tokens=max_vocab_length,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6078717d-6e41-402a-87c1-ef4159ec8a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Fit the text vectorizer to the training text\n",
    "text_vectorizer.adapt(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fd91662-fee4-41cc-a143-5760546e061b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[264,   3, 232,   4,  13, 698,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]])>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sample sentence and tokenize it\n",
    "sample_sentence = \"There's a flood in my street!\"\n",
    "text_vectorizer([sample_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14bd1d50-9449-4e66-834e-f44ccdb161c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@News@ Refugio oil spill may have been costlier bigger than projected http://t.co/SqoA1Wv4Um\n",
      "tf.Tensor([ 58 877 254 385 133  24  59 903 825  76 837   1   0   0   0], shape=(15,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Choose a random sentece from the training dataset and tokenize it\n",
    "random_sentence = random.choice(train_data)\n",
    "print(random_sentence)\n",
    "print(text_vectorizer([random_sentence])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "100aec62-446b-4307-9970-1d8e4cf39463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocab: 10000\n",
      "Top 5 words: ['', '[UNK]', 'the', 'a', 'in']\n",
      "Bottom 5 words: ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1']\n"
     ]
    }
   ],
   "source": [
    "# Get the unique words in the vocabulary\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "top_5_words = words_in_vocab[:5]\n",
    "bottom_5_words = words_in_vocab[-5:]\n",
    "print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n",
    "print(f\"Top 5 words: {top_5_words}\")\n",
    "print(f\"Bottom 5 words: {bottom_5_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0d7a21-223f-4b19-bf73-56fd9e5d0d8d",
   "metadata": {},
   "source": [
    "### Creating an Embedding using Embedding layer\n",
    "\n",
    "The parameters we care mose about for our embedding layer:\n",
    "* `input_dim`\n",
    "* `output_dim`\n",
    "* `input_length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33fbdcba-e9c4-4e3d-a3fd-6ba070d4b12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f903f9e-6192-45ce-8e60-7af943479a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.core.embedding.Embedding at 0x7e3abf321990>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = layers.Embedding(\n",
    "    input_dim=max_vocab_length,\n",
    "    output_dim=128,\n",
    "    input_length=max_length\n",
    ")\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bce59749-c61a-4712-be59-c4017ae324cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: @4Tiles @ZacB_ my dell tablet screams with win10\n",
      "Embedded version\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       "array([[[-0.0170662 , -0.0013448 ,  0.00620183, ...,  0.02611388,\n",
       "          0.01755878,  0.03428194],\n",
       "        [-0.03994006, -0.01006335,  0.02618611, ..., -0.02584904,\n",
       "          0.04671277,  0.02936259],\n",
       "        [-0.0284097 ,  0.03395047,  0.04765326, ...,  0.00472718,\n",
       "          0.0389403 , -0.03218024],\n",
       "        ...,\n",
       "        [ 0.00619169, -0.02054888,  0.0458217 , ..., -0.00686262,\n",
       "         -0.02809798,  0.0353192 ],\n",
       "        [ 0.00619169, -0.02054888,  0.0458217 , ..., -0.00686262,\n",
       "         -0.02809798,  0.0353192 ],\n",
       "        [ 0.00619169, -0.02054888,  0.0458217 , ..., -0.00686262,\n",
       "         -0.02809798,  0.0353192 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a random sentence from the training set\n",
    "random_sentence = random.choice(train_data)\n",
    "print(f\"Original: {random_sentence}\")\n",
    "print(\"Embedded version\")\n",
    "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
    "sample_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a23e1c03-1dcc-43bd-8941-b9f663b37cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       " array([-0.0170662 , -0.0013448 ,  0.00620183,  0.03522669, -0.02471845,\n",
       "         0.00499241, -0.01772612,  0.01827887,  0.01512973, -0.00533117,\n",
       "         0.00894164,  0.04344299, -0.01255572, -0.04537147,  0.02263627,\n",
       "         0.01188263,  0.01251714,  0.02878357, -0.01814557, -0.03086179,\n",
       "         0.04161762,  0.01372885, -0.01977268,  0.02016485, -0.00124905,\n",
       "         0.04292705,  0.02907348,  0.02275864, -0.04898793,  0.02115886,\n",
       "         0.04670696, -0.04538723,  0.01527737,  0.02097887,  0.04436118,\n",
       "        -0.01981043,  0.02880431, -0.0118211 ,  0.00665926, -0.02832267,\n",
       "        -0.04112154, -0.04108057,  0.02765051,  0.00400851, -0.03617083,\n",
       "         0.0332124 ,  0.02616094,  0.0038197 ,  0.04721022,  0.01482211,\n",
       "         0.01103127, -0.04868431,  0.02679973, -0.04463741,  0.019147  ,\n",
       "         0.0300248 , -0.03053572,  0.04068763,  0.04965141, -0.03561594,\n",
       "         0.00856341,  0.03570865,  0.03270484, -0.00254396,  0.01988994,\n",
       "        -0.01712281, -0.00661342,  0.02327652,  0.0072214 , -0.04421714,\n",
       "        -0.03099669,  0.03422785,  0.00374857,  0.04381606,  0.00326028,\n",
       "         0.0321768 , -0.03399781,  0.0490179 ,  0.00123744,  0.00488727,\n",
       "         0.03687178, -0.01769312,  0.04715165,  0.02411599,  0.04776667,\n",
       "         0.0158766 , -0.02364345,  0.01376781, -0.04216909,  0.01245008,\n",
       "        -0.04338572,  0.02736214, -0.00720043,  0.03602778,  0.00870032,\n",
       "        -0.00371671, -0.04536578, -0.02191726,  0.02165843,  0.00886558,\n",
       "         0.00321441,  0.03959126, -0.01178037,  0.00836115, -0.03889598,\n",
       "        -0.03622985, -0.04779545, -0.04237728, -0.03816806, -0.03844224,\n",
       "        -0.02665167,  0.00571252, -0.04426953,  0.01449199, -0.03540168,\n",
       "        -0.03661672, -0.04968574,  0.01081575,  0.02828368, -0.03977304,\n",
       "         0.0136809 , -0.04422945, -0.03277525,  0.02587568,  0.02532179,\n",
       "         0.02611388,  0.01755878,  0.03428194], dtype=float32)>,\n",
       " TensorShape([128]),\n",
       " '@')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_embed[0][0], sample_embed[0][0].shape, random_sentence[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5708868d-554e-42ee-9fb6-f485741aadf5",
   "metadata": {},
   "source": [
    "## Modelling a text datasets (running a series of experimentes)\n",
    "\n",
    "Now we've a got way to turn our text sequences into numbers, it's time to start building a series of modelling experiments.\n",
    "\n",
    "We'll start with a baseline and move on from there.\n",
    "\n",
    "* **Model 0:** Naive Bayers (baseline)\n",
    "* **Model 1:** Feed-forward neural network (dense model)\n",
    "* **Model 2:** LSTM model (RNN)\n",
    "* **Model 3:** GRU Model (RNN)\n",
    "* **Model 4:** Bidirectional-LSTM model (RNN)\n",
    "* **Model 5:** 1D Convolutional Neural Network (CNN)\n",
    "* **Model 6:** TensorFlow Hub Pretrained Feature Extractor ( using transfer learning for NLP)\n",
    "* **Model 7:** Same as model 6 with 10% of data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d04ba25-891a-41c3-bb40-9bc142e154be",
   "metadata": {},
   "source": [
    "### Model 0: Getting a baseline\n",
    "\n",
    "To create our baseline, we'll use skleant's Multinomial Naive Bayers using the TF-IDF formula to convert our words to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c2d52f5-628c-44ad-a994-cd6d54013843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;MultinomialNB<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.naive_bayes.MultinomialNB.html\">?<span>Documentation for MultinomialNB</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>MultinomialNB()</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create tokenization and modelling pipeline\n",
    "model_0 = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the traiing data\n",
    "model_0.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f2a0b3d-2466-4598-a16e-bf23cd788288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate our base model\n",
    "baseline_score = model_0.score(val_data, val_labels)\n",
    "print(f\"Baseline accuracy: {baseline_score*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5c005b9-5808-4f48-8cb9-8d847b171b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make predictions\n",
    "baseline_preds = model_0.predict(val_data)\n",
    "baseline_preds[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a81c754-1207-40ea-a307-002d0020358c",
   "metadata": {},
   "source": [
    "### Creating and evaluation function for our model experiments\n",
    "\n",
    "Let's create function for:\n",
    "* Accuracy\n",
    "* Precision\n",
    "* Recall\n",
    "* F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29891ad8-3644-441c-949f-93a4189df175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate: accuracy, precision, recall and f1-score\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "886f356b-3c9d-4aca-8619-c7d494e061c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_results(y_true, y_pred):\n",
    "    # Calculate model accuracy\n",
    "    model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "    # Calculate model precision, recall and f1-score\n",
    "    model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "    model_results = {\n",
    "        \"accuracy\": model_accuracy,\n",
    "        \"precision\": model_precision,\n",
    "        \"recall\": model_recall,\n",
    "        \"f1\": model_f1\n",
    "    }\n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0420151-4acc-41c8-bb99-12dfb25b8840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.26509186351706,\n",
       " 'precision': 0.8111390004213173,\n",
       " 'recall': 0.7926509186351706,\n",
       " 'f1': 0.7862189758049549}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get baseline results\n",
    "baseline_results = calculate_results(val_labels, baseline_preds)\n",
    "baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9e2bae-308e-4e97-90c4-5263570fe125",
   "metadata": {},
   "source": [
    "### Model 1: A simple dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "302ad7ad-fb74-43e6-ab45-e7a670b6ef21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensorboard callback (if needed)\n",
    "# tensorboard_callback = create_tensorboard_callback(\"dirname\", \"experimentname\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b44d6e8-1dae-4292-ab1c-79ed8d4cbe5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 128)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1280129 (4.88 MB)\n",
      "Trainable params: 1280129 (4.88 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_1 = tf.keras.models.Model(inputs, outputs, name=\"model_1\")\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1e48cc46-849d-44f0-91db-796030bd44f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model_1.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e705eaa3-93df-41e1-bf97-3cf877ae6c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 7s 27ms/step - loss: 0.6120 - accuracy: 0.6891 - val_loss: 0.5389 - val_accuracy: 0.7454\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.4423 - accuracy: 0.8194 - val_loss: 0.4712 - val_accuracy: 0.7861\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.3469 - accuracy: 0.8628 - val_loss: 0.4556 - val_accuracy: 0.7900\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 1s 3ms/step - loss: 0.2846 - accuracy: 0.8892 - val_loss: 0.4657 - val_accuracy: 0.7913\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.2382 - accuracy: 0.9110 - val_loss: 0.4833 - val_accuracy: 0.7782\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_1_history = model_1.fit(\n",
    "    x=train_data,\n",
    "    y=train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_data, val_labels),\n",
    "    #callbacks=[create_tensorboard_callback(\"a\", \"b\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "badec79b-d177-4023-81a1-0cc44b6e0a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 985us/step - loss: 0.4833 - accuracy: 0.7782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4832908511161804, 0.778215229511261]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.evaluate(val_data,val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb4377e3-98a2-421b-8e08-284b8700c912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 785us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(762, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_pred_probs = model_1.predict(val_data)\n",
    "model_1_pred_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2ffd64f-1ac7-44e4-a270-ed8479b36002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36026943],\n",
       "       [0.78479403],\n",
       "       [0.9979735 ],\n",
       "       [0.2130078 ],\n",
       "       [0.10958496],\n",
       "       [0.9493257 ],\n",
       "       [0.9257107 ],\n",
       "       [0.99161386],\n",
       "       [0.9698693 ],\n",
       "       [0.38368794]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7eef75b0-9b2d-48f8-bd6e-71c4196ca17b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert model prediction probabilities to label format\n",
    "model_1_preds = tf.squeeze(tf.round(model_1_pred_probs))\n",
    "model_1_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "108b99cc-ff89-4a28-af03-a1ae0b076cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 77.82152230971128,\n",
       " 'precision': 0.7801652612118941,\n",
       " 'recall': 0.7782152230971129,\n",
       " 'f1': 0.7761410008648765}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate our model 1 results\n",
    "model_1_results = calculate_results(val_labels, model_1_preds)\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "203453ff-ce31-4ec3-9b71-cff24010cd85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.26509186351706,\n",
       " 'precision': 0.8111390004213173,\n",
       " 'recall': 0.7926509186351706,\n",
       " 'f1': 0.7862189758049549}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "61111264-5893-4106-821a-889963dbfedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(list(model_1_results.values())) > np.array(list(baseline_results.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc83658-2cde-43c3-94a1-bc40f5c6d77a",
   "metadata": {},
   "source": [
    "### Visualizing learned embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c6220e25-ac45-4ff6-bc91-73f28996e9e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, ['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the vocabulary from the text vectorization layer\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "len(words_in_vocab), words_in_vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b32e2e3c-b380-4ef7-a650-a208648bd031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 128)\n"
     ]
    }
   ],
   "source": [
    "# Get the weight matrix of embedding layer\n",
    "# (thses are the numerical representation of each token in our training data, which have been learned for ~5 epochs)\n",
    "embed_weights = model_1.get_layer(\"embedding\").get_weights()[0]\n",
    "print(embed_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568c141a-43d5-4e08-96df-cab23e0d2809",
   "metadata": {},
   "source": [
    "Now we've got the embedding matrix our model has learned to represent our tokens, let's see how we can visualize it.\n",
    "\n",
    "To do so, TensorFlow has a handy took called projector: http://projector.tensorflow.org/\n",
    "\n",
    "And TensorFlow also has an incredible guide on word embedding themselves: https://www.tensorflow.org/tutorials/text/word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f67f46b9-f8d5-41cd-a022-bcece4573832",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf metadata.csv\n",
    "!rm -rf vector.tsv\n",
    "\n",
    "# Create embedding files (we got this from TensorFlow's word embeddings documentation)\n",
    "import io\n",
    "\n",
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(words_in_vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = embed_weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60afd42-cbd2-4ad8-ba57-8b40a80fa943",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks (RNN's)\n",
    "\n",
    "RNN's are usefule for sequence data.\n",
    "\n",
    "The premise of a recurrent neural network is to use the representation of a previous input to aid the representation of a later input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7521679-b9d2-4311-bada-01700ac2e257",
   "metadata": {},
   "source": [
    "### Model 2: LSTM\n",
    "\n",
    "LSTM = Long short term memory (one of the most popular LSTM cells)\n",
    "\n",
    "Our structure of an RNN typically looks like this:\n",
    "\n",
    "```\n",
    "Input (text) -> Tokenize -> Embedding -> Layers (RNNs/densse) -> Output (label probability)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7aa73f97-1949-481b-aa46-c8639e8fc6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1329473 (5.07 MB)\n",
      "Trainable params: 1329473 (5.07 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create an LSTM model\n",
    "\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "#x = layers.LSTM(64, return_sequences=True)(x)\n",
    "x = layers.LSTM(64)(x) # When you're stacking RNN cells together, you need to set return_sequences=True \n",
    "#x = layers.Dense(64, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_2 = tf.keras.Model(inputs, outputs, name=\"model_2\")\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7da31062-d2f3-4f82-bbd1-d7bd1e20f6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model 2\n",
    "model_2.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2ca330c2-d23d-455d-a887-fca8d95520f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 9s 31ms/step - loss: 0.2197 - accuracy: 0.9222 - val_loss: 0.5617 - val_accuracy: 0.7795\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 2s 7ms/step - loss: 0.1575 - accuracy: 0.9404 - val_loss: 0.5925 - val_accuracy: 0.7900\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.1292 - accuracy: 0.9518 - val_loss: 0.8311 - val_accuracy: 0.7743\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.1062 - accuracy: 0.9597 - val_loss: 0.8325 - val_accuracy: 0.7822\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 1s 3ms/step - loss: 0.0833 - accuracy: 0.9666 - val_loss: 0.9387 - val_accuracy: 0.7835\n"
     ]
    }
   ],
   "source": [
    "# Fit model 2\n",
    "model_2_history = model_2.fit(\n",
    "    train_data,\n",
    "    train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_data, val_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d23d47d5-66ab-4cb0-8c49-e19be7743dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.0573566e-01],\n",
       "       [9.0737486e-01],\n",
       "       [9.9985206e-01],\n",
       "       [5.5217505e-02],\n",
       "       [7.6181279e-04],\n",
       "       [9.9952424e-01],\n",
       "       [9.7713667e-01],\n",
       "       [9.9990058e-01],\n",
       "       [9.9984181e-01],\n",
       "       [6.6004163e-01]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2_pred_probs = model_2.predict(val_data)\n",
    "model_2_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8e58706a-260b-4643-b4e3-e51de184f96f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2_preds = tf.squeeze(tf.round(model_2_pred_probs))\n",
    "model_2_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f72c62be-686f-4a64-b9cf-1cdbc9377990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 78.34645669291339,\n",
       " 'precision': 0.7831607280426178,\n",
       " 'recall': 0.7834645669291339,\n",
       " 'f1': 0.7830062872880695}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2_results = calculate_results(val_labels, model_2_preds)\n",
    "model_2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ee74322a-9dee-464e-a2b4-5c302d815583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.26509186351706,\n",
       " 'precision': 0.8111390004213173,\n",
       " 'recall': 0.7926509186351706,\n",
       " 'f1': 0.7862189758049549}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e5b6a6-1a87-435e-b3b2-b42bacdb9674",
   "metadata": {},
   "source": [
    "### Model 3: GRU\n",
    "\n",
    "Another popular and effetive RNN component is the GRU or gated recurrent unit.\n",
    "\n",
    "The GRU cells has similar features to a LSTM cell but has less parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b2540da0-0ca7-4679-ae1d-7719d5396106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 64)                37248     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1317313 (5.03 MB)\n",
      "Trainable params: 1317313 (5.03 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build a RNN using the GRU cell\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "#x = layers.GRU(64, return_sequences=True)(x) # if you want to stack you need to set return_sequences=True\n",
    "#x = layers.LSTM(64, return_sequences=True)(x)\n",
    "x = layers.GRU(64)(x)\n",
    "#x = layers.GlobalAveragePooling1D()(x)\n",
    "#x = layers.Dense(64, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_3 = tf.keras.Model(inputs, outputs, name=\"model_3\")\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cfe28bb1-ff69-4f40-a072-cb2d412dd110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model 3\n",
    "model_3.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "51886d9c-38dc-4025-9fb3-6a025be73809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 9s 31ms/step - loss: 0.1586 - accuracy: 0.9400 - val_loss: 0.6903 - val_accuracy: 0.7769\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.0812 - accuracy: 0.9707 - val_loss: 0.7091 - val_accuracy: 0.7664\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0692 - accuracy: 0.9731 - val_loss: 1.0091 - val_accuracy: 0.7769\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 1s 3ms/step - loss: 0.0627 - accuracy: 0.9746 - val_loss: 1.1229 - val_accuracy: 0.7795\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.0525 - accuracy: 0.9772 - val_loss: 1.0829 - val_accuracy: 0.7756\n"
     ]
    }
   ],
   "source": [
    "# Fit model 3\n",
    "model_3_history = model_3.fit(\n",
    "    train_data,\n",
    "    train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_data, val_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c322a2f6-8064-4770-8015-0e9a31569742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3_pred_probs = model_3.predict(val_data)\n",
    "model_3_preds = tf.squeeze(tf.round(model_3_pred_probs))\n",
    "model_3_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ec2c088f-49a6-42ed-b99a-86cad02178c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'accuracy': 77.55905511811024,\n",
       "  'precision': 0.7772070861555818,\n",
       "  'recall': 0.7755905511811023,\n",
       "  'f1': 0.7736182129212565},\n",
       " {'accuracy': 79.26509186351706,\n",
       "  'precision': 0.8111390004213173,\n",
       "  'recall': 0.7926509186351706,\n",
       "  'f1': 0.7862189758049549})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3_results = calculate_results(val_labels, model_3_preds)\n",
    "model_3_results, baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bec2b9-cfc2-405d-b26b-0b96269886fa",
   "metadata": {},
   "source": [
    "### Model 4: Bidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4e594c76-1d17-43eb-aedd-73ccea474449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 128)               98816     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1378945 (5.26 MB)\n",
      "Trainable params: 1378945 (5.26 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build a bidirectional RNN\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "#x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_4 = tf.keras.Model(inputs, outputs)\n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9811e256-42c2-4556-a40d-45aaa476ccea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model 4\n",
    "model_4.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3b9393ef-e551-4122-b6b6-f2d6ea86398f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 11s 31ms/step - loss: 0.1048 - accuracy: 0.9705 - val_loss: 0.9623 - val_accuracy: 0.7690\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.0510 - accuracy: 0.9787 - val_loss: 1.1871 - val_accuracy: 0.7782\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.0452 - accuracy: 0.9793 - val_loss: 1.4075 - val_accuracy: 0.7677\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.0409 - accuracy: 0.9806 - val_loss: 1.4522 - val_accuracy: 0.7677\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.0396 - accuracy: 0.9803 - val_loss: 1.4834 - val_accuracy: 0.7638\n"
     ]
    }
   ],
   "source": [
    "# Fit model 4\n",
    "model_4_history = model_4.fit(\n",
    "    train_data,\n",
    "    train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_data, val_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e5adcfc3-3f0c-45d0-bbb5-73d70f3ea017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 0., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4_preds = tf.squeeze(tf.round(model_4.predict(val_data)))\n",
    "model_4_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4215f213-0fa3-4b1a-86bc-783ff79529f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'accuracy': 76.37795275590551,\n",
       "  'precision': 0.764383846466808,\n",
       "  'recall': 0.7637795275590551,\n",
       "  'f1': 0.7621412379223813},\n",
       " {'accuracy': 79.26509186351706,\n",
       "  'precision': 0.8111390004213173,\n",
       "  'recall': 0.7926509186351706,\n",
       "  'f1': 0.7862189758049549})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4_results = calculate_results(val_labels, model_4_preds)\n",
    "model_4_results, baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a5a5cb-2410-4f43-b20c-4baec09d25ab",
   "metadata": {},
   "source": [
    "### Convolution Neural Networks for Text (and other types of sequences)\n",
    "\n",
    "We've used CNNs for images but images are typically 2D (height x width)... however, our text data is 1D.\n",
    "\n",
    "Previously we've used Conv2D forour image data but now we're going to use Conv1D.\n",
    "\n",
    "The typical structure of a Conv1D model for squences (in our case, text):\n",
    "\n",
    "```\n",
    "Inputs (text) -> Tokenization -> Embedding -> Layer(s) (typically Conv1D + pooling) -> Outputs (class probabilities)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cf9f45-c29b-44da-bd8d-c8ccae21f26c",
   "metadata": {},
   "source": [
    "### Model 5: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1b07a9cb-0b87-4da4-b213-8d38815f3712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 11, 64)            41024     \n",
      "                                                                 \n",
      " global_max_pooling1d (Glob  (None, 64)                0         \n",
      " alMaxPooling1D)                                                 \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1321089 (5.04 MB)\n",
      "Trainable params: 1321089 (5.04 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build model 5\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.Conv1D(64, 5, activation=\"relu\", padding=\"valid\")(x)\n",
    "x = layers.GlobalMaxPool1D()(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_5 = tf.keras.Model(inputs, outputs)\n",
    "model_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "719e7fb5-5ab6-4fde-9fa3-b18dd14be88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model 5\n",
    "model_5.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "843a92fe-57ac-4a41-b6c8-6be8e808350e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 8s 30ms/step - loss: 0.1208 - accuracy: 0.9578 - val_loss: 0.9551 - val_accuracy: 0.7664\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.0756 - accuracy: 0.9712 - val_loss: 1.0135 - val_accuracy: 0.7572\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.0627 - accuracy: 0.9740 - val_loss: 1.1816 - val_accuracy: 0.7507\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.0539 - accuracy: 0.9781 - val_loss: 1.3302 - val_accuracy: 0.7612\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.0522 - accuracy: 0.9774 - val_loss: 1.2667 - val_accuracy: 0.7480\n"
     ]
    }
   ],
   "source": [
    "# Fit model 5\n",
    "model_5_history = model_5.fit(\n",
    "    train_data,\n",
    "    train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_data, val_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1efb5f01-46d8-4f75-bd80-195019f9dbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 861us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5_preds = tf.squeeze(tf.round(model_5.predict(val_data)))\n",
    "model_5_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "24a752ed-6ab4-4e9b-9b02-396e1298bdf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'accuracy': 74.80314960629921,\n",
       "  'precision': 0.7483325878156181,\n",
       "  'recall': 0.7480314960629921,\n",
       "  'f1': 0.7462839871172066},\n",
       " {'accuracy': 79.26509186351706,\n",
       "  'precision': 0.8111390004213173,\n",
       "  'recall': 0.7926509186351706,\n",
       "  'f1': 0.7862189758049549})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5_results = calculate_results(val_labels, model_5_preds)\n",
    "model_5_results, baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611c571c-752c-4287-8450-1970acf712eb",
   "metadata": {},
   "source": [
    "### Model 6: TensorFlow Hub Pretrained Sentece encoder\n",
    "\n",
    "Using universal sentence encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3b705a04-efde-42b5-b750-a31007740ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f25f1cc5-a35a-4f4b-bf45-ab9de4928483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[-0.01157025  0.02485909  0.0287805  -0.01271501  0.0397154   0.08827759\n",
      "  0.02680984  0.05589838 -0.0106873  -0.00597292], shape=(10,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "embed_samples = embed([sample_sentence])\n",
    "print(embed_samples[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6c06fe9d-850d-47ef-b148-66161d64046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Keras Layer using the pretrained USE (universal sentence encoder) layer from tensorflow hub\n",
    "sentence_encoder_layer = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "    input_shape=[],\n",
    "    dtype=tf.string,\n",
    "    trainable=False,\n",
    "    name=\"universal_sentence_encoder\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6c69127f-2d4e-4781-ae83-5b864e1df4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " universal_sentence_encoder  (None, 512)               256797824 \n",
      "  (KerasLayer)                                                   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256863617 (979.86 MB)\n",
      "Trainable params: 65793 (257.00 KB)\n",
      "Non-trainable params: 256797824 (979.61 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_6 = tf.keras.Sequential([\n",
    "    sentence_encoder_layer,\n",
    "    layers.Dense(128, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "], name=\"model_6\")\n",
    "\n",
    "model_6.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model_6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "44c1a709-0fa2-441a-af9d-97c10837e5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 5s 7ms/step - loss: 0.4910 - accuracy: 0.7843 - val_loss: 0.4469 - val_accuracy: 0.8058\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.4095 - accuracy: 0.8202 - val_loss: 0.4431 - val_accuracy: 0.7992\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.3949 - accuracy: 0.8305 - val_loss: 0.4405 - val_accuracy: 0.8045\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.3837 - accuracy: 0.8305 - val_loss: 0.4289 - val_accuracy: 0.8215\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.3728 - accuracy: 0.8377 - val_loss: 0.4223 - val_accuracy: 0.8215\n"
     ]
    }
   ],
   "source": [
    "model_6_history = model_6.fit(\n",
    "    train_data,\n",
    "    train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_data, val_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a7f263e2-f949-4d88-8077-304d30e7d107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'accuracy': 82.1522309711286,\n",
       "  'precision': 0.8235164537152119,\n",
       "  'recall': 0.821522309711286,\n",
       "  'f1': 0.8201933058918384},\n",
       " {'accuracy': 79.26509186351706,\n",
       "  'precision': 0.8111390004213173,\n",
       "  'recall': 0.7926509186351706,\n",
       "  'f1': 0.7862189758049549})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_6_preds = tf.squeeze(tf.round(model_6.predict(val_data)))\n",
    "model_6_results = calculate_results(val_labels, model_6_preds)\n",
    "model_6_results, baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea099d3b-b103-48d6-b7e2-c5ef9fb665c1",
   "metadata": {},
   "source": [
    "### Model 7: TF Hub Pretrained USE but with 10% off training data\n",
    "\n",
    "Transfer learning really helps when you don't have a large dataset.\n",
    "\n",
    "To see how our model performs on a smaller dataset, let's replicate `model_6` except we'll trainn it on 10% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a6ee3627-e437-448e-8adf-3c36cc4a7165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(685, 685)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create subsets of 10% of the training data\n",
    "train_10_percent_split = int(0.1 * len(train_data))\n",
    "\n",
    "train_data_10_percent = train_data[:train_10_percent_split]\n",
    "train_labels_10_percent = train_labels[:train_10_percent_split]\n",
    "len(train_data_10_percent), len(train_labels_10_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ba973ff2-bd03-4b65-9882-05ddd4e7b909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    4342\n",
       "1    3271\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_shuffled[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a6607e59-5d2c-4210-a38d-d00be662947f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " universal_sentence_encoder  (None, 512)               256797824 \n",
      "  (KerasLayer)                                                   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256863617 (979.86 MB)\n",
      "Trainable params: 65793 (257.00 KB)\n",
      "Non-trainable params: 256797824 (979.61 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_7 = tf.keras.models.clone_model(model_6)\n",
    "\n",
    "model_7.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model_7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "06aa35fd-d539-4b54-a27b-d705806c0895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "22/22 [==============================] - 4s 34ms/step - loss: 0.6642 - accuracy: 0.7182 - val_loss: 0.6345 - val_accuracy: 0.7467\n",
      "Epoch 2/5\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.5725 - accuracy: 0.8029 - val_loss: 0.5639 - val_accuracy: 0.7717\n",
      "Epoch 3/5\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4832 - accuracy: 0.8146 - val_loss: 0.5079 - val_accuracy: 0.7808\n",
      "Epoch 4/5\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4241 - accuracy: 0.8292 - val_loss: 0.4854 - val_accuracy: 0.7861\n",
      "Epoch 5/5\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3878 - accuracy: 0.8394 - val_loss: 0.4830 - val_accuracy: 0.7795\n"
     ]
    }
   ],
   "source": [
    "model_7_history = model_7.fit(\n",
    "    train_data_10_percent,\n",
    "    train_labels_10_percent,\n",
    "    epochs=5,\n",
    "    validation_data=(val_data, val_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6bafa282-eb6d-42d7-bd1d-3db1d388c584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'accuracy': 77.95275590551181,\n",
       "  'precision': 0.7802422774076316,\n",
       "  'recall': 0.7795275590551181,\n",
       "  'f1': 0.7781078501550943},\n",
       " {'accuracy': 79.26509186351706,\n",
       "  'precision': 0.8111390004213173,\n",
       "  'recall': 0.7926509186351706,\n",
       "  'f1': 0.7862189758049549})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_7_preds = tf.squeeze(tf.round(model_7.predict(val_data)))\n",
    "model_7_results = calculate_results(val_labels, model_7_preds)\n",
    "model_7_results, baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f3034e-f85c-4058-8c11-f8b1b39f4868",
   "metadata": {},
   "source": [
    "## Comparing the performance of each of our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "81f160f4-277a-4524-9db9-541ca3b42b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model_0</th>\n",
       "      <td>79.265092</td>\n",
       "      <td>0.811139</td>\n",
       "      <td>0.792651</td>\n",
       "      <td>0.786219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1</th>\n",
       "      <td>77.821522</td>\n",
       "      <td>0.780165</td>\n",
       "      <td>0.778215</td>\n",
       "      <td>0.776141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_2</th>\n",
       "      <td>78.346457</td>\n",
       "      <td>0.783161</td>\n",
       "      <td>0.783465</td>\n",
       "      <td>0.783006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_3</th>\n",
       "      <td>77.559055</td>\n",
       "      <td>0.777207</td>\n",
       "      <td>0.775591</td>\n",
       "      <td>0.773618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_4</th>\n",
       "      <td>76.377953</td>\n",
       "      <td>0.764384</td>\n",
       "      <td>0.763780</td>\n",
       "      <td>0.762141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_5</th>\n",
       "      <td>74.803150</td>\n",
       "      <td>0.748333</td>\n",
       "      <td>0.748031</td>\n",
       "      <td>0.746284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_6</th>\n",
       "      <td>82.152231</td>\n",
       "      <td>0.823516</td>\n",
       "      <td>0.821522</td>\n",
       "      <td>0.820193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_7</th>\n",
       "      <td>77.952756</td>\n",
       "      <td>0.780242</td>\n",
       "      <td>0.779528</td>\n",
       "      <td>0.778108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          accuracy  precision    recall        f1\n",
       "model_0  79.265092   0.811139  0.792651  0.786219\n",
       "model_1  77.821522   0.780165  0.778215  0.776141\n",
       "model_2  78.346457   0.783161  0.783465  0.783006\n",
       "model_3  77.559055   0.777207  0.775591  0.773618\n",
       "model_4  76.377953   0.764384  0.763780  0.762141\n",
       "model_5  74.803150   0.748333  0.748031  0.746284\n",
       "model_6  82.152231   0.823516  0.821522  0.820193\n",
       "model_7  77.952756   0.780242  0.779528  0.778108"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_results = pd.DataFrame({\n",
    "        f\"model_{i}\": {**results} for i, results in enumerate([\n",
    "        baseline_results,\n",
    "        model_1_results,\n",
    "        model_2_results,\n",
    "        model_3_results,\n",
    "        model_4_results,\n",
    "        model_5_results,\n",
    "        model_6_results,\n",
    "        model_7_results\n",
    "    ])\n",
    "}).transpose()\n",
    "\n",
    "models_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cb32fb97-4531-4d0d-9343-45ca19e262f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the accuracy to the same scale as other metrics\n",
    "models_results[\"accuracy\"] = models_results[\"accuracy\"]/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b4dc13be-bffc-4bd9-9c5c-ea7eceb1f592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7e39544a72e0>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models_results.transpose().plot(kind=\"barh\", figsize=(10, 7)).legend(bbox_to_anchor=(1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2200e505-a520-40fb-9fb9-8996ce3da0ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sort model results by f1-score\n",
    "models_results.sort_values(\"f1\", ascending=True)[\"f1\"].plot(kind=\"barh\", figsize=(10, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea3ebdf-1d31-4208-902e-beeb604c3425",
   "metadata": {},
   "source": [
    "## Saving and loading a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "43e8d5c3-1125-4242-b16c-c887304694d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save TF Hub Sentence Encoder model to HDF5 format\n",
    "model_6.save(\"savedmodels/model_6.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cd016a77-1bda-41ad-94fa-afa7b54c0854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "# Load model with custom Hub Layer (required HDF5 format)\n",
    "loaded_model_6_hdf5 = tf.keras.models.load_model(\n",
    "    \"savedmodels/model_6.h5\",\n",
    "    custom_objects={\n",
    "        \"KerasLayer\": hub.KerasLayer\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d13dce22-77d2-45d8-9f9c-c638fedba257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 4ms/step - loss: 0.4223 - accuracy: 0.8215\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4223065972328186, 0.8215222954750061]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How does our loaded model perform?\n",
    "loaded_model_6_hdf5.evaluate(val_data, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a109aea0-536c-4b89-916f-2fdf9373343d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 4ms/step - loss: 0.4223 - accuracy: 0.8215\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4223065972328186, 0.8215222954750061]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_6.evaluate(val_data, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "80040ae6-11fe-445f-901f-03cef2320f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: savedmodels/model_6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: savedmodels/model_6/assets\n"
     ]
    }
   ],
   "source": [
    "# Save TF Hub Sentence Encoder model to Savedmodel format\n",
    "model_6.save(\"savedmodels/model_6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "19caa86a-aca2-47ff-9e45-5b0ec9db5e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_6_sm = tf.keras.models.load_model(\"savedmodels/model_6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "661291af-997a-4b4d-a0e7-c214ad7e35b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 4ms/step - loss: 0.4223 - accuracy: 0.8215\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4223065972328186, 0.8215222954750061]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model_6_sm.evaluate(val_data, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262ac047-7a7d-476e-937d-c6a5eae0c997",
   "metadata": {},
   "source": [
    "## Finding the most wrong examples\n",
    "\n",
    "* If our model still isn't perfect, what examples is it getting wrong?\n",
    "* And of these wrong example which ones is it getting most wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ea49c8ad-93aa-4857-8ab9-0b93b735f0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-03 14:21:15--  https://storage.googleapis.com/ztm_tf_course/08_model_6_USE_feature_extractor.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 216.58.210.187, 216.58.211.251, 216.58.210.155, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|216.58.210.187|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 960779165 (916M) [application/zip]\n",
      "Saving to: ‘08_model_6_USE_feature_extractor.zip’\n",
      "\n",
      "08_model_6_USE_feat 100%[===================>] 916.27M  30.6MB/s    in 34s     \n",
      "\n",
      "2024-11-03 14:21:49 (27.1 MB/s) - ‘08_model_6_USE_feature_extractor.zip’ saved [960779165/960779165]\n",
      "\n",
      "Archive:  08_model_6_USE_feature_extractor.zip\n",
      "   creating: 08_model_6_USE_feature_extractor/\n",
      "   creating: 08_model_6_USE_feature_extractor/assets/\n",
      "   creating: 08_model_6_USE_feature_extractor/variables/\n",
      "  inflating: 08_model_6_USE_feature_extractor/variables/variables.data-00000-of-00001  \n",
      "  inflating: 08_model_6_USE_feature_extractor/variables/variables.index  \n",
      "  inflating: 08_model_6_USE_feature_extractor/saved_model.pb  \n"
     ]
    }
   ],
   "source": [
    "!rm -rf 08_model_6_USE_feature_extractor\n",
    "!wget https://storage.googleapis.com/ztm_tf_course/08_model_6_USE_feature_extractor.zip\n",
    "!unzip 08_model_6_USE_feature_extractor.zip\n",
    "!rm -rf 08_model_6_USE_feature_extractor.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "733acbd1-816c-4aa9-aca2-b2539f0f07fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 4ms/step - loss: 0.4272 - accuracy: 0.8163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.42723122239112854, 0.8162729740142822]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_6_pretrained = tf.keras.models.load_model(\"08_model_6_USE_feature_extractor\")\n",
    "model_6_pretrained.evaluate(val_data, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "06206a15-984f-4d00-82a5-46e4a6cbca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions with the loaded model from gs\n",
    "model_6_pretrained_probs = model_6_pretrained(val_data)\n",
    "model_6_pretrained_preds = tf.squeeze(tf.round(model_6_pretrained_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "47e31ea7-191d-49d9-9d30-1269119fda7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>pred</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DFR EP016 Monthly Meltdown - On Dnbheaven 2015...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.680485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FedEx no longer to transport bioterror germs i...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.494324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gunmen kill four in El Salvador bus attack: Su...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.977497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@camilacabello97 Internally and externally scr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.607541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Radiation emergency #preparedness starts with ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.415616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  pred      prob\n",
       "0  DFR EP016 Monthly Meltdown - On Dnbheaven 2015...       0   0.0  0.680485\n",
       "1  FedEx no longer to transport bioterror germs i...       0   1.0  0.494324\n",
       "2  Gunmen kill four in El Salvador bus attack: Su...       1   1.0  0.977497\n",
       "3  @camilacabello97 Internally and externally scr...       1   0.0  0.607541\n",
       "4  Radiation emergency #preparedness starts with ...       1   1.0  0.415616"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataFrame with validation sentences and best perfroming model predictions\n",
    "val_df = pd.DataFrame({\n",
    "    \"text\": val_data,\n",
    "    \"target\": val_labels,\n",
    "    \"pred\": model_6_pretrained_preds,\n",
    "    \"prob\": tf.squeeze(model_6_pretrained_probs)\n",
    "})\n",
    "val_df[\"prob\"] = abs(val_df[\"prob\"] - 0.5) * 2\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "79084c3a-721b-49e8-9fc4-a085dc8e54a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>pred</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Ron &amp;amp; Fez - Dave's High School Crush https...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.925628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>Reddit Will Now QuarantineÛ_ http://t.co/pkUA...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.922101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Why are you deluged with low self-image? Take ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.922004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>I get to smoke my shit in peace</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.915826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>@SoonerMagic_ I mean I'm a fan but I don't nee...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.912163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>I moved to England five years ago today. What ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>#computers #gadgets Two giant cranes holding a...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Back from Seattle Tacoma and Portland. Whirlwi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The Five Fatal Flaws in the Iran Deal https://...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.008688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>@Dirk_NoMissSki yea but if someone faints why ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  target  pred      prob\n",
       "23   Ron &amp; Fez - Dave's High School Crush https...       1   0.0  0.925628\n",
       "244  Reddit Will Now QuarantineÛ_ http://t.co/pkUA...       1   0.0  0.922101\n",
       "38   Why are you deluged with low self-image? Take ...       1   0.0  0.922004\n",
       "233                    I get to smoke my shit in peace       1   0.0  0.915826\n",
       "411  @SoonerMagic_ I mean I'm a fan but I don't nee...       1   0.0  0.912163\n",
       "..                                                 ...     ...   ...       ...\n",
       "659  I moved to England five years ago today. What ...       1   0.0  0.014895\n",
       "335  #computers #gadgets Two giant cranes holding a...       1   0.0  0.014429\n",
       "69   Back from Seattle Tacoma and Portland. Whirlwi...       1   0.0  0.009959\n",
       "11   The Five Fatal Flaws in the Iran Deal https://...       0   1.0  0.008688\n",
       "541  @Dirk_NoMissSki yea but if someone faints why ...       1   0.0  0.004287\n",
       "\n",
       "[140 rows x 4 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the wrong predictions and sort by prediction probabilities\n",
    "most_wrong = val_df[val_df[\"target\"] != val_df[\"pred\"]].sort_values(\"prob\", ascending=False)\n",
    "most_wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1f3a1e-fa3a-4413-bf41-a8b4534402ba",
   "metadata": {},
   "source": [
    "## Making predictions on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fd25b59f-d672-43ac-91ad-1ca44d16b3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 446ms/step\n",
      "Pred: 0, Prob: 0.13164283335208893\n",
      "Text:\n",
      "*screams internally*\n",
      "\n",
      "--------\n",
      "\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Pred: 1, Prob: 0.7850059866905212\n",
      "Text:\n",
      "IF SUICIDE BOMBING WASTHE SMARTEST THING2 DO FOR ALLAH/GODJESUS/THE HOLY PROPHET MUHAMMAD COULD HAVE KILLEDSOMEBODY? http://t.co/tGfWuVVHxj\n",
      "\n",
      "--------\n",
      "\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Pred: 0, Prob: 0.22350972890853882\n",
      "Text:\n",
      "@SavanahResnik @CBS12 I would hide out at the Coldstone at monterrey and us 1. Great place to wait out a rainstorm.\n",
      "\n",
      "--------\n",
      "\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Pred: 1, Prob: 0.9385698437690735\n",
      "Text:\n",
      "'Police Officer Wounded Suspect Dead After Exchanging Shots' http://t.co/guNq7ZTUn4 #????_?????\n",
      "\n",
      "--------\n",
      "\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Pred: 0, Prob: 0.05317344143986702\n",
      "Text:\n",
      "Wrecked today got my hattrick ????\n",
      "\n",
      "--------\n",
      "\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Pred: 0, Prob: 0.37393516302108765\n",
      "Text:\n",
      "Now Playing Desolation Wilderness by Kodak To Graph\n",
      "\n",
      "--------\n",
      "\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Pred: 1, Prob: 0.9699962139129639\n",
      "Text:\n",
      "Palestinian rams car into Israeli soldiers wounding 3 http://t.co/n79BMXQKlg #EMM\n",
      "\n",
      "--------\n",
      "\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Pred: 0, Prob: 0.09445924311876297\n",
      "Text:\n",
      "Oops! I say he picked the right car to try hijacking.  https://t.co/SoMgI7hwli\n",
      "\n",
      "--------\n",
      "\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Pred: 1, Prob: 0.9332718849182129\n",
      "Text:\n",
      "#gaming Learning from the Legacy of a Catastrophic Eruption: fifteen-mile-high plume of ash . The eruption wh... http://t.co/sv5KBP1FmO\n",
      "\n",
      "--------\n",
      "\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Pred: 1, Prob: 0.8557348847389221\n",
      "Text:\n",
      "Digital archive of public documents on the 2011 nuclear catastrophe at Fukushima launched. http://t.co/faJnMjNcxp\n",
      "\n",
      "--------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Making predictions on the test dataset and visualizing them\n",
    "test_sentences = test_df[\"text\"].to_list()\n",
    "test_samples = random.sample(test_sentences, 10)\n",
    "for ts in test_samples:\n",
    "    probs = tf.squeeze(model_6_pretrained.predict([ts]))\n",
    "    pred = tf.round(probs)\n",
    "    print(f\"Pred: {int(pred)}, Prob: {probs}\")\n",
    "    print(f\"Text:\\n{ts}\\n\")\n",
    "    print(\"--------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "357dea21-6818-4b0e-afaf-ee1fcb2de719",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf 08_model_6_USE_feature_extractor\n",
    "!rm -rf savedmodels\n",
    "!rm -rf helper_functions.py\n",
    "!rm -rf *.csv\n",
    "!rm -rf *.tsv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
