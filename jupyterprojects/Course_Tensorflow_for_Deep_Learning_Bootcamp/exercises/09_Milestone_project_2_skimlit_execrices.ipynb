{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d958974d-66eb-4378-992a-77204d31c51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "for gpu_instance in physical_devices: \n",
    "    tf.config.experimental.set_memory_growth(gpu_instance, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "365a3cdb-cfe7-4889-8c05-abed6390e45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'glove.6B*': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm -rf pubmed-rct\n",
    "!rm -rf skimlit_tribrid_model*\n",
    "!rm checkpoint\n",
    "!rm glove.6B*\n",
    "!rm saved_weights*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ae553c-27d0-4626-90eb-47153f975c20",
   "metadata": {},
   "source": [
    "# ðŸ›  09. Milestone Project 2: SkimLit ðŸ“„ðŸ”¥ Exercises\n",
    "\n",
    "1. Train `model_5` on all of the data in the training dataset for as many epochs until it stops improving. Since this might take a while, you might want to use:\n",
    "  * [`tf.keras.callbacks.ModelCheckpoint`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint) to save the model's best weights only.\n",
    "  * [`tf.keras.callbacks.EarlyStopping`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) to stop the model from training once the validation loss has stopped improving for ~3 epochs.\n",
    "2. Checkout the [Keras guide on using pretrained GloVe embeddings](https://keras.io/examples/nlp/pretrained_word_embeddings/). Can you get this working with one of our models?\n",
    "  * Hint: You'll want to incorporate it with a custom token [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer.\n",
    "  * It's up to you whether or not you fine-tune the GloVe embeddings or leave them frozen.\n",
    "3. Try replacing the TensorFlow Hub Universal Sentence Encoder pretrained  embedding for the [TensorFlow Hub BERT PubMed expert](https://tfhub.dev/google/experts/bert/pubmed/2) (a language model pretrained on PubMed texts) pretrained embedding. Does this effect results?\n",
    "  * Note: Using the BERT PubMed expert pretrained embedding requires an extra preprocessing step for sequences (as detailed in the [TensorFlow Hub guide](https://tfhub.dev/google/experts/bert/pubmed/2)).\n",
    "  * Does the BERT model beat the results mentioned in this paper? https://arxiv.org/pdf/1710.06071.pdf \n",
    "4. What happens if you were to merge our `line_number` and `total_lines` features for each sequence? For example, created a `X_of_Y` feature instead? Does this effect model performance?\n",
    "  * Another example: `line_number=1` and `total_lines=11` turns into `line_of_X=1_of_11`.\n",
    "5. Write a function (or series of functions) to take a sample abstract string, preprocess it (in the same way our model has been trained), make a prediction on each sequence in the abstract and return the abstract in the format:\n",
    "  * `PREDICTED_LABEL`: `SEQUENCE`\n",
    "  * `PREDICTED_LABEL`: `SEQUENCE`\n",
    "  * `PREDICTED_LABEL`: `SEQUENCE`\n",
    "  * `PREDICTED_LABEL`: `SEQUENCE`\n",
    "  * ...\n",
    "    * You can find your own unstructured RCT abstract from PubMed or try this one from: [*Baclofen promotes alcohol abstinence in alcohol dependent cirrhotic patients with hepatitis C virus (HCV) infection*](https://pubmed.ncbi.nlm.nih.gov/22244707/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39e5f7c-40ba-475d-8821-cc9c6abdabbd",
   "metadata": {},
   "source": [
    "## 1. Train `model_5` on all of the data in the training dataset for as many epochs until it stops improving. Since this might take a while, you might want to use:\n",
    "  * [`tf.keras.callbacks.ModelCheckpoint`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint) to save the model's best weights only.\n",
    "  * [`tf.keras.callbacks.EarlyStopping`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) to stop the model from training once the validation loss has stopped improving for ~3 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1ca285-cf95-4621-914f-fc8be234af04",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "034bd75f-511d-4f23-bdbd-18c9fa4fb9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'skimlit_tribrid_model.zip': No such file or directory\n",
      "--2024-11-07 19:17:34--  https://storage.googleapis.com/ztm_tf_course/skimlit/skimlit_tribrid_model.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 216.58.211.251, 216.58.209.187, 216.58.209.219, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|216.58.211.251|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 962182847 (918M) [application/zip]\n",
      "Saving to: â€˜skimlit_tribrid_model.zipâ€™\n",
      "\n",
      "skimlit_tribrid_mod 100%[===================>] 917.61M  32.5MB/s    in 28s     \n",
      "\n",
      "2024-11-07 19:18:02 (32.3 MB/s) - â€˜skimlit_tribrid_model.zipâ€™ saved [962182847/962182847]\n",
      "\n",
      "Archive:  skimlit_tribrid_model.zip\n",
      "   creating: skimlit_tribrid_model/\n",
      "  inflating: skimlit_tribrid_model/keras_metadata.pb  \n",
      "   creating: skimlit_tribrid_model/assets/\n",
      " extracting: skimlit_tribrid_model/fingerprint.pb  \n",
      "   creating: skimlit_tribrid_model/variables/\n",
      "  inflating: skimlit_tribrid_model/variables/variables.index  \n",
      "  inflating: skimlit_tribrid_model/variables/variables.data-00000-of-00001  \n",
      "  inflating: skimlit_tribrid_model/saved_model.pb  \n"
     ]
    }
   ],
   "source": [
    "!rm -rf skimlit_tribrid_model\n",
    "!rm skimlit_tribrid_model.zip\n",
    "!wget https://storage.googleapis.com/ztm_tf_course/skimlit/skimlit_tribrid_model.zip\n",
    "!unzip skimlit_tribrid_model.zip\n",
    "!rm skimlit_tribrid_model.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ff159f6-bbfa-4a41-93ab-6e0c10ae90ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model(\"skimlit_tribrid_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "924a36ea-a2ef-4053-beb3-fa3a5e17bec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " char_inputs (InputLayer)    [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " token_inputs (InputLayer)   [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " char_vectorizer (TextVecto  (None, 290)                  0         ['char_inputs[0][0]']         \n",
      " rization)                                                                                        \n",
      "                                                                                                  \n",
      " universal_sentence_encoder  (None, 512)                  2567978   ['token_inputs[0][0]']        \n",
      "  (KerasLayer)                                            24                                      \n",
      "                                                                                                  \n",
      " char_embed (Embedding)      (None, 290, 25)              1750      ['char_vectorizer[0][0]']     \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 128)                  65664     ['universal_sentence_encoder[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirecti  (None, 64)                   14848     ['char_embed[0][0]']          \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " token_char_hybrid_embeddin  (None, 192)                  0         ['dense_7[0][0]',             \n",
      " g (Concatenate)                                                     'bidirectional_1[0][0]']     \n",
      "                                                                                                  \n",
      " line_number_input (InputLa  [(None, 15)]                 0         []                            \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " total_lines_input (InputLa  [(None, 20)]                 0         []                            \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " dense_10 (Dense)            (None, 256)                  49408     ['token_char_hybrid_embedding[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 32)                   512       ['line_number_input[0][0]']   \n",
      "                                                                                                  \n",
      " dense_9 (Dense)             (None, 32)                   672       ['total_lines_input[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 256)                  0         ['dense_10[0][0]']            \n",
      "                                                                                                  \n",
      " token_char_positional_embe  (None, 320)                  0         ['dense_8[0][0]',             \n",
      " dding (Concatenate)                                                 'dense_9[0][0]',             \n",
      "                                                                     'dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      " output_layer (Dense)        (None, 5)                    1605      ['token_char_positional_embedd\n",
      "                                                                    ing[0][0]']                   \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 256932283 (980.12 MB)\n",
      "Trainable params: 134459 (525.23 KB)\n",
      "Non-trainable params: 256797824 (979.61 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc5de2d-f097-4f04-bc09-3e0f9cf5484b",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "642819a2-899a-4720-a92c-feebdd4a8f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  train.zip\n",
      "  inflating: train.txt               \n",
      "/home/jupyter/projects/Course_Tensorflow_for_Deep_Learning_Bootcamp/exercises\n"
     ]
    }
   ],
   "source": [
    "!rm -rf pubmed-rct\n",
    "!git clone --quiet https://github.com/Franck-Dernoncourt/pubmed-rct.git\n",
    "!cd pubmed-rct/PubMed_200k_RCT_numbers_replaced_with_at_sign/ && unzip train.zip && cd -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdafade-a4f2-4c3a-8ab3-97c621bd2a01",
   "metadata": {},
   "source": [
    "### Create training, validation and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26778f6a-11f9-40b6-badc-92eb4edba416",
   "metadata": {},
   "source": [
    "#### Read data into lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b235a56b-2f00-497d-882a-f72c742c6058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2593169, 34493, 33932)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"pubmed-rct/PubMed_200k_RCT_numbers_replaced_with_at_sign/\"\n",
    "# Create function to read the lines of a document\n",
    "def get_lines(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        return f.readlines()\n",
    "train_lines, test_lines, val_lines = (get_lines(f) for f in (f\"{data_dir}train.txt\", f\"{data_dir}test.txt\", f\"{data_dir}dev.txt\"))\n",
    "len(train_lines), len(test_lines), len(val_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7513b636-a57d-4af7-ab6e-a5417c983e50",
   "metadata": {},
   "source": [
    "#### Preprocess lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "034ac57d-9643-45f8-9845-40b4459521fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.81 s, sys: 480 ms, total: 4.29 s\n",
      "Wall time: 4.29 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2211861, 29493, 28932)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text_with_linenumbers(filename):\n",
    "    lines = get_lines(filename)\n",
    "    abstract_lines = \"\"\n",
    "    abstract_samples = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith(\"###\"):\n",
    "            abstract_lines = \"\"\n",
    "        elif line.isspace():\n",
    "            abstract_line_split = abstract_lines.splitlines()\n",
    "            for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n",
    "                target, text = abstract_line.split(\"\\t\", maxsplit=1)\n",
    "                abstract_samples.append({\n",
    "                    \"target\": target,\n",
    "                    \"text\": text,\n",
    "                    \"line_number\": abstract_line_number,\n",
    "                    \"total_lines\": len(abstract_line_split)-1\n",
    "                })\n",
    "        else:\n",
    "            abstract_lines += line\n",
    "\n",
    "    return abstract_samples\n",
    "\n",
    "%time train_samples, test_samples, val_samples = (preprocess_text_with_linenumbers(file) for file in (f\"{data_dir}train.txt\", f\"{data_dir}test.txt\", f\"{data_dir}dev.txt\"))\n",
    "len(train_samples), len(test_samples), len(val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdd3c094-2890-458b-ad27-1017dddcdd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "967c05ca-f57e-41d3-911c-2b9fe7a548cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>line_number</th>\n",
       "      <th>total_lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td>The emergence of HIV as a chronic condition me...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td>This paper describes the design and evaluation...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>This study is designed as a randomised control...</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>The intervention group will participate in the...</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>The program is based on self-efficacy theory a...</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       target                                               text  line_number  \\\n",
       "0  BACKGROUND  The emergence of HIV as a chronic condition me...            0   \n",
       "1  BACKGROUND  This paper describes the design and evaluation...            1   \n",
       "2     METHODS  This study is designed as a randomised control...            2   \n",
       "3     METHODS  The intervention group will participate in the...            3   \n",
       "4     METHODS  The program is based on self-efficacy theory a...            4   \n",
       "\n",
       "   total_lines  \n",
       "0           10  \n",
       "1           10  \n",
       "2           10  \n",
       "3           10  \n",
       "4           10  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df, val_df = (pd.DataFrame(samples) for samples in (train_samples, test_samples, val_samples))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b0c7d7-9ca5-42aa-ad7c-ea8427ba5613",
   "metadata": {},
   "source": [
    "#### Create training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47f65530-8ac0-424d-bec1-cb9d10afa032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "train_labels_one_hot = one_hot_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1,1))\n",
    "train_line_numbers_one_hot = tf.one_hot(train_df[\"line_number\"].to_numpy(), depth=15)\n",
    "train_total_lines_one_hot = tf.one_hot(train_df[\"total_lines\"].to_numpy(), depth=20)\n",
    "\n",
    "# Make function to split sentences into characters\n",
    "def split_chars(text):\n",
    "  return \" \".join(list(text))\n",
    "\n",
    "train_sentences = train_df[\"text\"].to_numpy()\n",
    "train_chars = [split_chars(sentence) for sentence in train_sentences]\n",
    "\n",
    "train_char_token_pos_data = tf.data.Dataset.from_tensor_slices((\n",
    "    train_line_numbers_one_hot,\n",
    "    train_total_lines_one_hot,\n",
    "    train_sentences,\n",
    "    train_chars\n",
    "))\n",
    "train_char_token_pos_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\n",
    "train_char_token_pos_dataset = tf.data.Dataset.zip((\n",
    "    train_char_token_pos_data,\n",
    "    train_char_token_pos_labels\n",
    ")).batch(1024).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403c3569-2331-4ff0-856c-6c8c2e13a56d",
   "metadata": {},
   "source": [
    "#### Create validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d02d404-6ccd-4732-ab22-1a4963758c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels_one_hot = one_hot_encoder.fit_transform(val_df[\"target\"].to_numpy().reshape(-1,1))\n",
    "val_line_numbers_one_hot = tf.one_hot(val_df[\"line_number\"].to_numpy(), depth=15)\n",
    "val_total_lines_one_hot = tf.one_hot(val_df[\"total_lines\"].to_numpy(), depth=20)\n",
    "\n",
    "# Make function to split sentences into characters\n",
    "def split_chars(text):\n",
    "  return \" \".join(list(text))\n",
    "\n",
    "val_sentences = val_df[\"text\"].to_numpy()\n",
    "val_chars = [split_chars(sentence) for sentence in val_sentences]\n",
    "\n",
    "val_char_token_pos_data = tf.data.Dataset.from_tensor_slices((\n",
    "    val_line_numbers_one_hot,\n",
    "    val_total_lines_one_hot,\n",
    "    val_sentences,\n",
    "    val_chars\n",
    "))\n",
    "val_char_token_pos_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
    "val_char_token_pos_dataset = tf.data.Dataset.zip((\n",
    "    val_char_token_pos_data,\n",
    "    val_char_token_pos_labels\n",
    ")).batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7d11af-3cea-455e-8046-d8d19a678209",
   "metadata": {},
   "source": [
    "#### Create test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34b6170b-6fb8-47bf-ac76-49a81717a02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_one_hot = one_hot_encoder.fit_transform(test_df[\"target\"].to_numpy().reshape(-1,1))\n",
    "test_line_numbers_one_hot = tf.one_hot(test_df[\"line_number\"].to_numpy(), depth=15)\n",
    "test_total_lines_one_hot = tf.one_hot(test_df[\"total_lines\"].to_numpy(), depth=20)\n",
    "\n",
    "# Make function to split sentences into characters\n",
    "def split_chars(text):\n",
    "  return \" \".join(list(text))\n",
    "\n",
    "test_sentences = test_df[\"text\"].to_numpy()\n",
    "test_chars = [split_chars(sentence) for sentence in test_sentences]\n",
    "\n",
    "test_char_token_pos_data = tf.data.Dataset.from_tensor_slices((\n",
    "    test_line_numbers_one_hot,\n",
    "    test_total_lines_one_hot,\n",
    "    test_sentences,\n",
    "    test_chars\n",
    "))\n",
    "test_char_token_pos_labels = tf.data.Dataset.from_tensor_slices(test_labels_one_hot)\n",
    "test_char_token_pos_dataset = tf.data.Dataset.zip((\n",
    "    test_char_token_pos_data,\n",
    "    test_char_token_pos_labels\n",
    ")).batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae10dbb9-6dd2-442b-9997-8c85f0fba1bd",
   "metadata": {},
   "source": [
    "### Create Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c9cfc72-88cd-4784-bfab-c4045f190b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dir = \"saved_weights\"\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(weights_dir, save_best_only=True, save_weights_only=True)\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(patience=1, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591bfaa3-fedf-489b-88cd-7d8c294a1b98",
   "metadata": {},
   "source": [
    "### Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34025318-c587-4279-aafb-53fb0b180060",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013176b1-6319-47a2-a789-f7ecd5f5a510",
   "metadata": {},
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3687a07d-941d-4140-8fe9-5aab908c4430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2161/2161 [==============================] - 210s 93ms/step - loss: 0.3628 - accuracy: 0.8633 - val_loss: 0.3296 - val_accuracy: 0.8716\n",
      "Epoch 2/100\n",
      "2161/2161 [==============================] - 199s 92ms/step - loss: 0.3199 - accuracy: 0.8806 - val_loss: 0.3019 - val_accuracy: 0.8837\n",
      "Epoch 3/100\n",
      "2161/2161 [==============================] - 198s 92ms/step - loss: 0.3085 - accuracy: 0.8853 - val_loss: 0.2947 - val_accuracy: 0.8865\n",
      "Epoch 4/100\n",
      "2161/2161 [==============================] - 198s 91ms/step - loss: 0.3022 - accuracy: 0.8877 - val_loss: 0.2909 - val_accuracy: 0.8884\n",
      "Epoch 5/100\n",
      "2161/2161 [==============================] - 198s 91ms/step - loss: 0.2993 - accuracy: 0.8888 - val_loss: 0.2879 - val_accuracy: 0.8899\n",
      "Epoch 6/100\n",
      "2161/2161 [==============================] - 199s 92ms/step - loss: 0.2971 - accuracy: 0.8896 - val_loss: 0.2864 - val_accuracy: 0.8909\n",
      "Epoch 7/100\n",
      "2161/2161 [==============================] - 198s 91ms/step - loss: 0.2932 - accuracy: 0.8912 - val_loss: 0.2838 - val_accuracy: 0.8914\n",
      "Epoch 8/100\n",
      "2161/2161 [==============================] - 199s 92ms/step - loss: 0.2911 - accuracy: 0.8921 - val_loss: 0.2828 - val_accuracy: 0.8927\n",
      "Epoch 9/100\n",
      "2161/2161 [==============================] - 198s 92ms/step - loss: 0.2890 - accuracy: 0.8928 - val_loss: 0.2826 - val_accuracy: 0.8915\n",
      "Epoch 10/100\n",
      "2161/2161 [==============================] - 198s 92ms/step - loss: 0.2875 - accuracy: 0.8935 - val_loss: 0.2809 - val_accuracy: 0.8929\n",
      "Epoch 11/100\n",
      "2161/2161 [==============================] - 199s 92ms/step - loss: 0.2857 - accuracy: 0.8940 - val_loss: 0.2801 - val_accuracy: 0.8935\n",
      "Epoch 12/100\n",
      "2161/2161 [==============================] - 197s 91ms/step - loss: 0.2842 - accuracy: 0.8945 - val_loss: 0.2795 - val_accuracy: 0.8938\n",
      "Epoch 13/100\n",
      "2161/2161 [==============================] - 196s 91ms/step - loss: 0.2831 - accuracy: 0.8949 - val_loss: 0.2835 - val_accuracy: 0.8923\n",
      "905/905 [==============================] - 5s 6ms/step - loss: 0.2795 - accuracy: 0.8938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.27947017550468445, 0.8937508463859558]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_history = model.fit(\n",
    "    train_char_token_pos_dataset,\n",
    "    epochs=100,\n",
    "    validation_data=val_char_token_pos_dataset,\n",
    "    callbacks=[model_checkpoint_callback, early_stopping_callback]\n",
    ")\n",
    "model_0_score = model.evaluate(val_char_token_pos_dataset)\n",
    "model_0_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b28a1f7f-72cf-46d2-b7ef-dbaa587516db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "905/905 [==============================] - 5s 6ms/step - loss: 0.2795 - accuracy: 0.8938\n"
     ]
    }
   ],
   "source": [
    "model_score = model.evaluate(val_char_token_pos_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ade03931-c28a-4352-bfd2-80ebc622faa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.27947017550468445, 0.8937508463859558]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04425d54-cf4e-4de1-92c3-5582b0464ef7",
   "metadata": {},
   "source": [
    "## 2. Checkout the [Keras guide on using pretrained GloVe embeddings](https://keras.io/examples/nlp/pretrained_word_embeddings/). Can you get this working with one of our models?\n",
    "  * Hint: You'll want to incorporate it with a custom token [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer.\n",
    "  * It's up to you whether or not you fine-tune the GloVe embeddings or leave them frozen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e10877-bd94-487b-8ac1-320b5deb2a3e",
   "metadata": {},
   "source": [
    "### Create TextVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da3405e4-bdfb-4190-9c1b-20efedeca53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['The emergence of HIV as a chronic condition means that people living with HIV are required to take more responsibility for the self-management of their condition , including making physical , emotional and social adjustments .',\n",
       "       'This paper describes the design and evaluation of Positive Outlook , an online program aiming to enhance the self-management skills of gay men living with HIV .',\n",
       "       'This study is designed as a randomised controlled trial in which men living with HIV in Australia will be assigned to either an intervention group or usual care control group .',\n",
       "       \"The intervention group will participate in the online group program ` Positive Outlook ' .\",\n",
       "       'The program is based on self-efficacy theory and uses a self-management approach to enhance skills , confidence and abilities to manage the psychosocial issues associated with HIV in daily life .'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "196e63ff-c684-4d6b-8cda-b1ce36ba29c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "output_seq_len = int(np.percentile([len(s.split(\" \")) for s in train_sentences], 95))\n",
    "vectorizer = tf.keras.layers.TextVectorization(max_tokens=20000, output_sequence_length=output_seq_len)\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(train_sentences).batch(1024*4).prefetch(tf.data.AUTOTUNE)\n",
    "vectorizer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c33a116-3193-47b7-a98d-0f6a7a9b545d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'of', 'and']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))\n",
    "voc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fec19e-c722-4f6c-83c0-ac76e8c46140",
   "metadata": {},
   "source": [
    "### Get Glove6B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46297501-0ca1-41f4-a2f1-02a754d02789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'glove.6B.zip': No such file or directory\n",
      "--2024-11-07 20:03:07--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: â€˜glove.6B.zipâ€™\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 40s  \n",
      "\n",
      "2024-11-07 20:05:48 (5.15 MB/s) - â€˜glove.6B.zipâ€™ saved [862182613/862182613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm glove.6B.zip\n",
    "!rm -rf glove.6B\n",
    "!wget https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
    "!unzip -q glove.6B.zip\n",
    "!rm glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccb1087-a141-4b7c-a11d-0e94b021a1b2",
   "metadata": {},
   "source": [
    "### Create weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5434a705-26ba-4ca2-a7f3-1d9123a48a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "Converted 15399 words (4601 misses)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "path_to_glove_file = \"glove.6B.100d.txt\"\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))\n",
    "\n",
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45df19e2-2e7c-433d-bb59-dc236347ca97",
   "metadata": {},
   "source": [
    "### Create layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75a77d83-d87f-43c0-983c-ef7e1b659c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    trainable=False,\n",
    ")\n",
    "embedding_layer.build((1,))\n",
    "embedding_layer.set_weights([embedding_matrix])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4dc1ef-ba6c-4a10-8a70-b69822382057",
   "metadata": {},
   "source": [
    "### Create CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9c23424-5ebc-4ba7-8ab3-843164651558",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "# Create model\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "x = vectorizer(inputs)\n",
    "x = embedding_layer(x)\n",
    "x = layers.Conv1D(64, 5, activation=\"relu\", padding=\"valid\")(x)\n",
    "x = layers.GlobalMaxPool1D()(x)\n",
    "outputs = layers.Dense(5, activation=\"softmax\")(x)\n",
    "\n",
    "model_1 = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95b2aee-fcba-4d81-bbc2-2e6390793951",
   "metadata": {},
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1814aedb-ba1c-4979-95e0-9a241d636f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7264b70e-ccbe-49a7-a959-f1eb6e09a397",
   "metadata": {},
   "source": [
    "### Create dataset for CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3fc8d520-e4b7-43bf-bc25-41a946472cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_one_hot)).batch(2048).prefetch(tf.data.AUTOTUNE)\n",
    "# Validation data\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "# Test data\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2c32e8-3a4c-41fc-8950-62873a9eb2e2",
   "metadata": {},
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "437fc6a6-ced9-4d11-987b-9ae93b47e703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1081/1081 [==============================] - 19s 16ms/step - loss: 0.6331 - accuracy: 0.7669 - val_loss: 0.5919 - val_accuracy: 0.7728\n",
      "Epoch 2/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 0.5159 - accuracy: 0.8117 - val_loss: 0.5267 - val_accuracy: 0.8030\n",
      "Epoch 3/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 0.4951 - accuracy: 0.8197 - val_loss: 0.5076 - val_accuracy: 0.8103\n",
      "Epoch 4/100\n",
      "1081/1081 [==============================] - 18s 16ms/step - loss: 0.4845 - accuracy: 0.8236 - val_loss: 0.5003 - val_accuracy: 0.8132\n",
      "Epoch 5/100\n",
      "1081/1081 [==============================] - 18s 16ms/step - loss: 0.4780 - accuracy: 0.8259 - val_loss: 0.4944 - val_accuracy: 0.8154\n",
      "Epoch 6/100\n",
      "1081/1081 [==============================] - 18s 16ms/step - loss: 0.4734 - accuracy: 0.8275 - val_loss: 0.4930 - val_accuracy: 0.8144\n",
      "Epoch 7/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 0.4700 - accuracy: 0.8289 - val_loss: 0.4907 - val_accuracy: 0.8151\n",
      "Epoch 8/100\n",
      "1081/1081 [==============================] - 18s 16ms/step - loss: 0.4674 - accuracy: 0.8298 - val_loss: 0.4866 - val_accuracy: 0.8181\n",
      "Epoch 9/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 0.4652 - accuracy: 0.8307 - val_loss: 0.4858 - val_accuracy: 0.8182\n",
      "Epoch 10/100\n",
      "1081/1081 [==============================] - 18s 16ms/step - loss: 0.4635 - accuracy: 0.8313 - val_loss: 0.4839 - val_accuracy: 0.8189\n",
      "Epoch 11/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 0.4620 - accuracy: 0.8318 - val_loss: 0.4827 - val_accuracy: 0.8198\n",
      "Epoch 12/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 0.4607 - accuracy: 0.8322 - val_loss: 0.4828 - val_accuracy: 0.8200\n",
      "905/905 [==============================] - 1s 1ms/step - loss: 0.4827 - accuracy: 0.8198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.482700377702713, 0.819818913936615]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_history = model_1.fit(\n",
    "  train_dataset,\n",
    "  epochs=100,\n",
    "  validation_data=val_dataset,\n",
    "  callbacks=[early_stopping_callback]\n",
    ")\n",
    "model_1_score = model_1.evaluate(val_dataset)\n",
    "model_1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14a70204-bee3-42a8-9e3d-4d637a1c2222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.27947017550468445, 0.8937508463859558],\n",
       " [0.482700377702713, 0.819818913936615])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_score, model_1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfe2813-a1c5-4f3e-a161-e31ba9c68823",
   "metadata": {},
   "source": [
    "## 3. Try replacing the TensorFlow Hub Universal Sentence Encoder pretrained  embedding for the [TensorFlow Hub BERT PubMed expert](https://tfhub.dev/google/experts/bert/pubmed/2) (a language model pretrained on PubMed texts) pretrained embedding. Does this effect results?\n",
    "  * Note: Using the BERT PubMed expert pretrained embedding requires an extra preprocessing step for sequences (as detailed in the [TensorFlow Hub guide](https://tfhub.dev/google/experts/bert/pubmed/2)).\n",
    "  * Does the BERT model beat the results mentioned in this paper? https://arxiv.org/pdf/1710.06071.pdf "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51a90ce-d567-46cf-8eed-9fdcfc9d60dd",
   "metadata": {},
   "source": [
    "### Get bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe3091dd-aafb-4219-8f94-28ea8be50065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "\n",
    "preprocess = hub.KerasLayer('https://kaggle.com/models/tensorflow/bert/TensorFlow2/en-uncased-preprocess/3', trainable = False)\n",
    "bert = hub.KerasLayer('https://www.kaggle.com/models/google/experts-bert/TensorFlow2/pubmed/2', trainable = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d95a4ec-051c-4423-a0f5-860fb209f441",
   "metadata": {},
   "source": [
    "### Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3d12675-7864-4a38-bc26-9eff01016c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 546 ms, sys: 52.5 ms, total: 598 ms\n",
      "Wall time: 598 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>line_number</th>\n",
       "      <th>total_lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OBJECTIVE</td>\n",
       "      <td>To investigate the efficacy of @ weeks of dail...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>A total of @ patients with primary knee OA wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>Outcome measures included pain reduction and i...</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>Pain was assessed using the visual analog pain...</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>Secondary outcome measures included the Wester...</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      target                                               text  line_number  \\\n",
       "0  OBJECTIVE  To investigate the efficacy of @ weeks of dail...            0   \n",
       "1    METHODS  A total of @ patients with primary knee OA wer...            1   \n",
       "2    METHODS  Outcome measures included pain reduction and i...            2   \n",
       "3    METHODS  Pain was assessed using the visual analog pain...            3   \n",
       "4    METHODS  Secondary outcome measures included the Wester...            4   \n",
       "\n",
       "   total_lines  \n",
       "0           11  \n",
       "1           11  \n",
       "2           11  \n",
       "3           11  \n",
       "4           11  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_dir = \"pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/\"\n",
    "# Create function to read the lines of a document\n",
    "def get_lines(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        return f.readlines()\n",
    "train_lines, test_lines, val_lines = (get_lines(f) for f in (f\"{data_dir}train.txt\", f\"{data_dir}test.txt\", f\"{data_dir}dev.txt\"))\n",
    "\n",
    "def preprocess_text_with_linenumbers(filename):\n",
    "    lines = get_lines(filename)\n",
    "    abstract_lines = \"\"\n",
    "    abstract_samples = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith(\"###\"):\n",
    "            abstract_lines = \"\"\n",
    "        elif line.isspace():\n",
    "            abstract_line_split = abstract_lines.splitlines()\n",
    "            for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n",
    "                target, text = abstract_line.split(\"\\t\", maxsplit=1)\n",
    "                abstract_samples.append({\n",
    "                    \"target\": target,\n",
    "                    \"text\": text,\n",
    "                    \"line_number\": abstract_line_number,\n",
    "                    \"total_lines\": len(abstract_line_split)-1\n",
    "                })\n",
    "        else:\n",
    "            abstract_lines += line\n",
    "\n",
    "    return abstract_samples\n",
    "\n",
    "%time train_samples, test_samples, val_samples = (preprocess_text_with_linenumbers(file) for file in (f\"{data_dir}train.txt\", f\"{data_dir}test.txt\", f\"{data_dir}dev.txt\"))\n",
    "len(train_samples), len(test_samples), len(val_samples)\n",
    "\n",
    "train_df, test_df, val_df = (pd.DataFrame(samples) for samples in (train_samples, test_samples, val_samples))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df38d6c-0040-4641-b960-fbc95d25a774",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7ff41ecc-d755-4eaf-a9ed-dc4c85b83e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "train_labels_one_hot = one_hot_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1,1))\n",
    "train_line_numbers_one_hot = tf.one_hot(train_df[\"line_number\"].to_numpy(), depth=15)\n",
    "train_total_lines_one_hot = tf.one_hot(train_df[\"total_lines\"].to_numpy(), depth=20)\n",
    "\n",
    "# Make function to split sentences into characters\n",
    "def split_chars(text):\n",
    "  return \" \".join(list(text))\n",
    "\n",
    "train_sentences = train_df[\"text\"].to_numpy()\n",
    "train_chars = [split_chars(sentence) for sentence in train_sentences]\n",
    "\n",
    "train_char_token_pos_data = tf.data.Dataset.from_tensor_slices((\n",
    "    train_line_numbers_one_hot,\n",
    "    train_total_lines_one_hot,\n",
    "    train_sentences,\n",
    "    train_chars\n",
    "))\n",
    "train_char_token_pos_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\n",
    "train_char_token_pos_dataset = tf.data.Dataset.zip((\n",
    "    train_char_token_pos_data,\n",
    "    train_char_token_pos_labels\n",
    ")).batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6d1c4e-e94c-4b3b-9518-526f79756a0c",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cccb6442-ac87-48ca-9c6f-eec76c199f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels_one_hot = one_hot_encoder.fit_transform(val_df[\"target\"].to_numpy().reshape(-1,1))\n",
    "val_line_numbers_one_hot = tf.one_hot(val_df[\"line_number\"].to_numpy(), depth=15)\n",
    "val_total_lines_one_hot = tf.one_hot(val_df[\"total_lines\"].to_numpy(), depth=20)\n",
    "\n",
    "val_sentences = val_df[\"text\"].to_numpy()\n",
    "val_chars = [split_chars(sentence) for sentence in val_sentences]\n",
    "\n",
    "val_char_token_pos_data = tf.data.Dataset.from_tensor_slices((\n",
    "    val_line_numbers_one_hot,\n",
    "    val_total_lines_one_hot,\n",
    "    val_sentences,\n",
    "    val_chars\n",
    "))\n",
    "val_char_token_pos_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
    "val_char_token_pos_dataset = tf.data.Dataset.zip((\n",
    "    val_char_token_pos_data,\n",
    "    val_char_token_pos_labels\n",
    ")).batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5423d95f-16d3-42bc-8466-1b4e127c4096",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d9b0630-1daa-4f6f-b577-45e91f718dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_one_hot = one_hot_encoder.fit_transform(test_df[\"target\"].to_numpy().reshape(-1,1))\n",
    "test_line_numbers_one_hot = tf.one_hot(test_df[\"line_number\"].to_numpy(), depth=15)\n",
    "test_total_lines_one_hot = tf.one_hot(test_df[\"total_lines\"].to_numpy(), depth=20)\n",
    "\n",
    "test_sentences = test_df[\"text\"].to_numpy()\n",
    "test_chars = [split_chars(sentence) for sentence in test_sentences]\n",
    "\n",
    "test_char_token_pos_data = tf.data.Dataset.from_tensor_slices((\n",
    "    test_line_numbers_one_hot,\n",
    "    test_total_lines_one_hot,\n",
    "    test_sentences,\n",
    "    test_chars\n",
    "))\n",
    "test_char_token_pos_labels = tf.data.Dataset.from_tensor_slices(test_labels_one_hot)\n",
    "test_char_token_pos_dataset = tf.data.Dataset.zip((\n",
    "    test_char_token_pos_data,\n",
    "    test_char_token_pos_labels\n",
    ")).batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3a780a-5b10-43d1-a895-1722de7291d4",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2577c0ae-d07a-4256-bf40-1a413fff8d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "# Buidling the tribid model using the functional api \n",
    "\n",
    "input_token = layers.Input(shape = [] , dtype =tf.string)\n",
    "bert_inputs_token = preprocess(input_token)\n",
    "bert_embedding_char = bert(bert_inputs_token)\n",
    "output_token = layers.Dense(64 , activation = 'relu')(bert_embedding_char['pooled_output'])\n",
    "token_model = tf.keras.Model(input_token , output_token)\n",
    "\n",
    "input_char = layers.Input(shape = [] , dtype =tf.string)\n",
    "bert_inputs_char = preprocess(input_char)\n",
    "bert_embedding_char = bert(bert_inputs_char)\n",
    "output_char = layers.Dense(64 , activation = 'relu')(bert_embedding_char['pooled_output'])\n",
    "char_model = tf.keras.Model(input_char , output_char)\n",
    "\n",
    "# 3. Line number model\n",
    "line_num_inputs = layers.Input(shape=(15,), dtype=tf.float32, name=\"line_number_input\")\n",
    "x = layers.Dense(32, activation=\"relu\")(line_num_inputs)\n",
    "line_number_model = tf.keras.Model(line_num_inputs, x)\n",
    "\n",
    "# 4. Total line model\n",
    "total_line_inputs = layers.Input(shape=(20,), dtype=tf.float32, name=\"total_lines_input\")\n",
    "x = layers.Dense(32, activation=\"relu\")(total_line_inputs)\n",
    "total_lines_models = tf.keras.Model(total_line_inputs, x)\n",
    "\n",
    "# Concatenating the tokens amd chars output (Hybrid!!!)\n",
    "combined_embeddings = layers.Concatenate(name = 'token_char_hybrid_embedding')([token_model.output , \n",
    "                                                                                char_model.output])\n",
    "\n",
    "# Combining the line_number_total to our hybrid model (Time for Tribid!!)\n",
    "z = layers.Concatenate(name = 'tribid_embeddings')([total_lines_models.output , \n",
    "                                                    combined_embeddings])\n",
    "\n",
    "# Adding a dense + dropout and creating our output layer \n",
    "dropout = layers.Dropout(0.5)(z)\n",
    "x = layers.Dense(128 , activation='relu')(dropout)\n",
    "output_layer = layers.Dense(5 , activation='softmax')(x)\n",
    "\n",
    "# Packing into a model\n",
    "model_2 = tf.keras.Model(inputs = [\n",
    "        line_number_model.input,\n",
    "        total_lines_models.input,\n",
    "        token_model.input,\n",
    "        char_model.input\n",
    "], \n",
    "outputs = output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d332dcd4-954b-4940-8b77-4cd22e646d54",
   "metadata": {},
   "source": [
    "### Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15b9dd4b-2325-4ae9-9b7c-d6dea48d3fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.compile(\n",
    "  loss=\"categorical_crossentropy\",\n",
    "  optimizer=tf.keras.optimizers.Adam(),\n",
    "  metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9730cf92-5672-4644-9d34-c1e95e06616b",
   "metadata": {},
   "source": [
    "### Fit the bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0480a45c-f2e2-4354-94e0-b4ff8a232d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "562/562 [==============================] - 184s 302ms/step - loss: 0.6914 - accuracy: 0.7553 - val_loss: 0.4146 - val_accuracy: 0.8524\n",
      "Epoch 2/100\n",
      "562/562 [==============================] - 168s 300ms/step - loss: 0.5365 - accuracy: 0.8157 - val_loss: 0.4171 - val_accuracy: 0.8351\n",
      "945/945 [==============================] - 273s 289ms/step - loss: 0.4499 - accuracy: 0.8410\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4498974680900574, 0.8409572243690491]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(patience=1, restore_best_weights=True)\n",
    "\n",
    "bert_history = model_2.fit(\n",
    "  train_char_token_pos_dataset,\n",
    "  epochs=100,\n",
    "  steps_per_epoch=int(len(train_char_token_pos_dataset) * 0.1),\n",
    "  validation_data=val_char_token_pos_dataset,\n",
    "  validation_steps=int(len(val_char_token_pos_dataset) * 0.02),\n",
    "  callbacks=[early_stopping_callback]\n",
    ")\n",
    "model_2_score = model_2.evaluate(val_char_token_pos_dataset)\n",
    "model_2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4dc048-7a33-4129-a30b-2ce135f93487",
   "metadata": {},
   "source": [
    "## 4. What happens if you were to merge our `line_number` and `total_lines` features for each sequence? For example, created a `X_of_Y` feature instead? Does this effect model performance?\n",
    "  * Another example: `line_number=1` and `total_lines=11` turns into `line_of_X=1_of_11`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ada1cc-6257-475b-9aaa-79fd19079e80",
   "metadata": {},
   "source": [
    "### Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a6eded63-97b7-4a5b-b15c-e367f6ee77cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 418 ms, sys: 6.85 ms, total: 425 ms\n",
      "Wall time: 425 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>line_number</th>\n",
       "      <th>total_lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OBJECTIVE</td>\n",
       "      <td>To investigate the efficacy of @ weeks of dail...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>A total of @ patients with primary knee OA wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>Outcome measures included pain reduction and i...</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>Pain was assessed using the visual analog pain...</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>Secondary outcome measures included the Wester...</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      target                                               text  line_number  \\\n",
       "0  OBJECTIVE  To investigate the efficacy of @ weeks of dail...            0   \n",
       "1    METHODS  A total of @ patients with primary knee OA wer...            1   \n",
       "2    METHODS  Outcome measures included pain reduction and i...            2   \n",
       "3    METHODS  Pain was assessed using the visual analog pain...            3   \n",
       "4    METHODS  Secondary outcome measures included the Wester...            4   \n",
       "\n",
       "   total_lines  \n",
       "0           11  \n",
       "1           11  \n",
       "2           11  \n",
       "3           11  \n",
       "4           11  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_dir = \"pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/\"\n",
    "# Create function to read the lines of a document\n",
    "def get_lines(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        return f.readlines()\n",
    "train_lines, test_lines, val_lines = (get_lines(f) for f in (f\"{data_dir}train.txt\", f\"{data_dir}test.txt\", f\"{data_dir}dev.txt\"))\n",
    "\n",
    "def preprocess_text_with_linenumbers(filename):\n",
    "    lines = get_lines(filename)\n",
    "    abstract_lines = \"\"\n",
    "    abstract_samples = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith(\"###\"):\n",
    "            abstract_lines = \"\"\n",
    "        elif line.isspace():\n",
    "            abstract_line_split = abstract_lines.splitlines()\n",
    "            for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n",
    "                target, text = abstract_line.split(\"\\t\", maxsplit=1)\n",
    "                abstract_samples.append({\n",
    "                    \"target\": target,\n",
    "                    \"text\": text,\n",
    "                    \"line_number\": abstract_line_number,\n",
    "                    \"total_lines\": len(abstract_line_split)-1\n",
    "                })\n",
    "        else:\n",
    "            abstract_lines += line\n",
    "\n",
    "    return abstract_samples\n",
    "\n",
    "%time train_samples, test_samples, val_samples = (preprocess_text_with_linenumbers(file) for file in (f\"{data_dir}train.txt\", f\"{data_dir}test.txt\", f\"{data_dir}dev.txt\"))\n",
    "len(train_samples), len(test_samples), len(val_samples)\n",
    "\n",
    "train_df, test_df, val_df = (pd.DataFrame(samples) for samples in (train_samples, test_samples, val_samples))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "154e6142-166d-4463-aa80-bc8efeae2234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>line_number</th>\n",
       "      <th>total_lines</th>\n",
       "      <th>line_number_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OBJECTIVE</td>\n",
       "      <td>To investigate the efficacy of @ weeks of dail...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0_of_11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>A total of @ patients with primary knee OA wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1_of_11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>Outcome measures included pain reduction and i...</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>2_of_11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>Pain was assessed using the visual analog pain...</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>3_of_11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>Secondary outcome measures included the Wester...</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>4_of_11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>Serum levels of interleukin @ ( IL-@ ) , IL-@ ...</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>5_of_11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>There was a clinically relevant reduction in t...</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>6_of_11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>The mean difference between treatment arms ( @...</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>7_of_11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>Further , there was a clinically relevant redu...</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>8_of_11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>These differences remained significant at @ we...</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>9_of_11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      target                                               text  line_number  \\\n",
       "0  OBJECTIVE  To investigate the efficacy of @ weeks of dail...            0   \n",
       "1    METHODS  A total of @ patients with primary knee OA wer...            1   \n",
       "2    METHODS  Outcome measures included pain reduction and i...            2   \n",
       "3    METHODS  Pain was assessed using the visual analog pain...            3   \n",
       "4    METHODS  Secondary outcome measures included the Wester...            4   \n",
       "5    METHODS  Serum levels of interleukin @ ( IL-@ ) , IL-@ ...            5   \n",
       "6    RESULTS  There was a clinically relevant reduction in t...            6   \n",
       "7    RESULTS  The mean difference between treatment arms ( @...            7   \n",
       "8    RESULTS  Further , there was a clinically relevant redu...            8   \n",
       "9    RESULTS  These differences remained significant at @ we...            9   \n",
       "\n",
       "   total_lines line_number_total  \n",
       "0           11           0_of_11  \n",
       "1           11           1_of_11  \n",
       "2           11           2_of_11  \n",
       "3           11           3_of_11  \n",
       "4           11           4_of_11  \n",
       "5           11           5_of_11  \n",
       "6           11           6_of_11  \n",
       "7           11           7_of_11  \n",
       "8           11           8_of_11  \n",
       "9           11           9_of_11  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combining the total lines and line number into a new feature! \n",
    "train_df['line_number_total'] = train_df['line_number'].astype(str) + '_of_' + train_df['total_lines'].astype(str)\n",
    "val_df['line_number_total'] = val_df['line_number'].astype(str) + '_of_' + val_df['total_lines'].astype(str)\n",
    "\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e27a18e9-19bc-4333-8d67-73d4202c0fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((180040, 460), (30212, 460))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform one hot encoding on the train and transform the validation dataframe \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Creating an instance \n",
    "one_hot_encoder = OneHotEncoder()\n",
    "\n",
    "# Fitting on the training dataframe \n",
    "one_hot_encoder.fit(np.expand_dims(train_df['line_number_total'] , axis = 1))\n",
    "\n",
    "# Transforming both train and val df \n",
    "train_line_number_total_encoded = one_hot_encoder.transform(np.expand_dims(train_df['line_number_total'] , axis =1))\n",
    "val_line_number_total_encoded  = one_hot_encoder.transform(np.expand_dims(val_df['line_number_total'] , axis= 1))\n",
    "\n",
    "# Checking the shapes \n",
    "train_line_number_total_encoded.shape , val_line_number_total_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b81d67e-5269-40b7-bee6-229ffbebc0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the sparse object to array \n",
    "train_line_number_total_encoded = train_line_number_total_encoded.toarray()\n",
    "val_line_number_total_encoded = val_line_number_total_encoded.toarray()\n",
    "\n",
    "# Converting the datatype to int \n",
    "train_line_number_total_encoded = tf.cast(train_line_number_total_encoded , dtype= tf.int32)\n",
    "val_line_number_total_encoded = tf.cast(val_line_number_total_encoded , dtype= tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ec1122-e5e9-4923-bc69-438376f30e1b",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c1583ad2-6c74-45ba-8a67-0a14e61f4f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "train_labels_one_hot = one_hot_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1,1))\n",
    "train_line_numbers_one_hot = tf.one_hot(train_df[\"line_number\"].to_numpy(), depth=15)\n",
    "train_total_lines_one_hot = tf.one_hot(train_df[\"total_lines\"].to_numpy(), depth=20)\n",
    "\n",
    "# Make function to split sentences into characters\n",
    "def split_chars(text):\n",
    "  return \" \".join(list(text))\n",
    "\n",
    "train_sentences = train_df[\"text\"].to_numpy()\n",
    "train_chars = [split_chars(sentence) for sentence in train_sentences]\n",
    "\n",
    "train_char_token_pos_data = tf.data.Dataset.from_tensor_slices((\n",
    "    train_line_number_total_encoded,\n",
    "    train_sentences,\n",
    "    train_chars\n",
    "))\n",
    "train_char_token_pos_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\n",
    "train_char_token_pos_dataset = tf.data.Dataset.zip((\n",
    "    train_char_token_pos_data,\n",
    "    train_char_token_pos_labels\n",
    ")).batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a42328-20d1-4329-a8ab-e1a5138bf4fd",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dd90be9e-9c41-437b-b96f-81205db41aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels_one_hot = one_hot_encoder.fit_transform(val_df[\"target\"].to_numpy().reshape(-1,1))\n",
    "val_line_numbers_one_hot = tf.one_hot(val_df[\"line_number\"].to_numpy(), depth=15)\n",
    "val_total_lines_one_hot = tf.one_hot(val_df[\"total_lines\"].to_numpy(), depth=20)\n",
    "\n",
    "val_sentences = val_df[\"text\"].to_numpy()\n",
    "val_chars = [split_chars(sentence) for sentence in val_sentences]\n",
    "\n",
    "val_char_token_pos_data = tf.data.Dataset.from_tensor_slices((\n",
    "    val_line_number_total_encoded,\n",
    "    val_sentences,\n",
    "    val_chars\n",
    "))\n",
    "val_char_token_pos_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
    "val_char_token_pos_dataset = tf.data.Dataset.zip((\n",
    "    val_char_token_pos_data,\n",
    "    val_char_token_pos_labels\n",
    ")).batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8925462-3f22-4e99-be77-1725becb1ed3",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2d32bdfc-e7fd-46c8-b2ce-dfe65d1316e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "preprocess = hub.KerasLayer('https://kaggle.com/models/tensorflow/bert/TensorFlow2/en-uncased-preprocess/3', trainable = False)\n",
    "bert = hub.KerasLayer('https://www.kaggle.com/models/google/experts-bert/TensorFlow2/pubmed/2', trainable = False)\n",
    "\n",
    "input_token = layers.Input(shape = [] , dtype =tf.string)\n",
    "bert_inputs_token = preprocess(input_token)\n",
    "bert_embedding_char = bert(bert_inputs_token)\n",
    "output_token = layers.Dense(64 , activation = 'relu')(bert_embedding_char['pooled_output'])\n",
    "token_model = tf.keras.Model(input_token , output_token)\n",
    "\n",
    "input_char = layers.Input(shape = [] , dtype =tf.string)\n",
    "bert_inputs_char = preprocess(input_char)\n",
    "bert_embedding_char = bert(bert_inputs_char)\n",
    "output_char = layers.Dense(64 , activation = 'relu')(bert_embedding_char['pooled_output'])\n",
    "char_model = tf.keras.Model(input_char , output_char)\n",
    "\n",
    "# 3. Line number model\n",
    "line_num_inputs = layers.Input(shape=(460,), dtype=tf.float32, name=\"line_number_input\")\n",
    "x = layers.Dense(32, activation=\"relu\")(line_num_inputs)\n",
    "line_number_model = tf.keras.Model(line_num_inputs, x)\n",
    "\n",
    "# Concatenating the tokens amd chars output (Hybrid!!!)\n",
    "combined_embeddings = layers.Concatenate(name = 'token_char_hybrid_embedding')([token_model.output , \n",
    "                                                                                char_model.output])\n",
    "\n",
    "# Combining the line_number_total to our hybrid model (Time for Tribid!!)\n",
    "z = layers.Concatenate(name = 'tribid_embeddings')([line_number_model.output , \n",
    "                                                    combined_embeddings])\n",
    "\n",
    "# Adding a dense + dropout and creating our output layer \n",
    "dropout = layers.Dropout(0.5)(z)\n",
    "x = layers.Dense(128 , activation='relu')(dropout)\n",
    "output_layer = layers.Dense(5 , activation='softmax')(x)\n",
    "\n",
    "# Packing into a model\n",
    "model_3 = tf.keras.Model(inputs = [\n",
    "        line_number_model.input,\n",
    "        token_model.input,\n",
    "        char_model.input\n",
    "], \n",
    "outputs = output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f2c965-6245-4af2-8329-982bb77eb7cd",
   "metadata": {},
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "179487cd-1f93-44c1-a409-db336bf7137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1684a61-c9b8-4ab6-a85c-73e289d53377",
   "metadata": {},
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "abb3c2dc-1ebd-4b29-b5b1-2a44ce8dc299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "562/562 [==============================] - 184s 302ms/step - loss: 0.6676 - accuracy: 0.7621 - val_loss: 0.3525 - val_accuracy: 0.8767\n",
      "Epoch 2/100\n",
      "562/562 [==============================] - 168s 299ms/step - loss: 0.4477 - accuracy: 0.8414 - val_loss: 0.3139 - val_accuracy: 0.8698\n",
      "Epoch 3/100\n",
      "562/562 [==============================] - 168s 299ms/step - loss: 0.4039 - accuracy: 0.8561 - val_loss: 0.3114 - val_accuracy: 0.8628\n",
      "Epoch 4/100\n",
      "562/562 [==============================] - 168s 299ms/step - loss: 0.3856 - accuracy: 0.8585 - val_loss: 0.2585 - val_accuracy: 0.8958\n",
      "Epoch 5/100\n",
      "562/562 [==============================] - 168s 299ms/step - loss: 0.3760 - accuracy: 0.8671 - val_loss: 0.2433 - val_accuracy: 0.9115\n",
      "Epoch 6/100\n",
      "562/562 [==============================] - 168s 299ms/step - loss: 0.3718 - accuracy: 0.8628 - val_loss: 0.2429 - val_accuracy: 0.9132\n",
      "Epoch 7/100\n",
      "562/562 [==============================] - 168s 299ms/step - loss: 0.3511 - accuracy: 0.8726 - val_loss: 0.2624 - val_accuracy: 0.9010\n",
      "945/945 [==============================] - 273s 289ms/step - loss: 0.2830 - accuracy: 0.8979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2829873263835907, 0.8978882431983948]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3_history = model_3.fit(\n",
    "    train_char_token_pos_dataset,\n",
    "    epochs=100,\n",
    "  steps_per_epoch=int(len(train_char_token_pos_dataset) * 0.1),\n",
    "    validation_data=val_char_token_pos_dataset,\n",
    "  validation_steps=int(len(val_char_token_pos_dataset) * 0.02),\n",
    "    callbacks=[early_stopping_callback]\n",
    ")\n",
    "model_3_score = model_3.evaluate(val_char_token_pos_dataset)\n",
    "model_3_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d57090fb-6e76-4332-b276-75a68106444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf pubmed-rct\n",
    "!rm -rf skimlit_tribrid_model*\n",
    "!rm checkpoint\n",
    "!rm glove.6B*\n",
    "!rm saved_weights*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbbbaaf-afb9-477e-968d-4aa1e844210b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
